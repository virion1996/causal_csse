{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "207c6680",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lingam in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.9.0)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from lingam) (1.26.4)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from lingam) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from lingam) (1.5.0)\n",
      "Requirement already satisfied: graphviz in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from lingam) (0.20.3)\n",
      "Requirement already satisfied: statsmodels in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from lingam) (0.14.2)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from lingam) (3.3)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from lingam) (2.2.2)\n",
      "Requirement already satisfied: pygam in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from lingam) (0.9.1)\n",
      "Requirement already satisfied: matplotlib in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from lingam) (3.8.4)\n",
      "Requirement already satisfied: psy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from lingam) (0.0.1)\n",
      "Requirement already satisfied: semopy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from lingam) (2.3.11)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib->lingam) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib->lingam) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib->lingam) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib->lingam) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib->lingam) (21.3)\n",
      "Requirement already satisfied: pillow>=8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib->lingam) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib->lingam) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib->lingam) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->lingam) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->lingam) (2024.1)\n",
      "Requirement already satisfied: progressbar2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from psy->lingam) (4.4.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-learn->lingam) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-learn->lingam) (3.5.0)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from semopy->lingam) (1.12)\n",
      "Requirement already satisfied: numdifftools in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from semopy->lingam) (0.9.41)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from statsmodels->lingam) (0.5.6)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from patsy>=0.5.6->statsmodels->lingam) (1.16.0)\n",
      "Requirement already satisfied: python-utils>=3.8.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from progressbar2->psy->lingam) (3.8.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sympy->semopy->lingam) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions>3.10.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-utils>=3.8.1->progressbar2->psy->lingam) (4.12.1)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (2024.6.0)\n",
      "Requirement already satisfied: s3fs in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (0.4.2)\n",
      "Requirement already satisfied: botocore>=1.12.91 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from s3fs) (1.34.142)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from s3fs) (2024.6.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore>=1.12.91->s3fs) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore>=1.12.91->s3fs) (2.9.0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore>=1.12.91->s3fs) (2.2.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.12.91->s3fs) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.system(\"pip install lingam\")\n",
    "os.system(\"pip install fsspec\")\n",
    "os.system(\"pip install s3fs\")\n",
    "\n",
    "import random as rnd\n",
    "from scipy.spatial import distance\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import warnings\n",
    "import sys\n",
    "import argparse\n",
    "import ast\n",
    "import time\n",
    "import json\n",
    "\n",
    "import boto3\n",
    "\n",
    "#German\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import graphviz\n",
    "import lingam\n",
    "from lingam.utils import print_causal_directions, print_dagc, make_dot\n",
    "\n",
    "import random as rnd\n",
    "\n",
    "# from IPython.display import display\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "#Used for ordering evaluations\n",
    "class individual:\n",
    "    def __init__(self, index, score, distance, num_changes, aval_norm, dist_norm, predict_proba):\n",
    "        self.index = index #Indicates the instance's position in the dataframe\n",
    "        self.score = score #Indicates the score in relation to the proximity of the class boundary\n",
    "        self.distance = distance #Indicates the distance from the original instance\n",
    "        self.num_changes = num_changes #Indicates the number of changes for class change\n",
    "        self.aval_norm = aval_norm #Indicates the final fitness with standardized metrics\n",
    "        self.dist_norm = dist_norm #Indicates the normalized distance (distance and number of changes)\n",
    "        self.predict_proba = predict_proba #Indicates de individual's class\n",
    "    def __repr__(self):\n",
    "        return repr((self.index, self.score, self.distance, self.num_changes, self.aval_norm, self.dist_norm, self.predict_proba))\n",
    "\n",
    "class counter_change:\n",
    "    def __init__(self, column, value):\n",
    "        self.column = column \n",
    "        self.value = value\n",
    "    def __eq__(self, other):\n",
    "        if self.column == other.column and self.value == other.value:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    def __repr__(self):\n",
    "        return repr((self.column, self.value))    \n",
    "\n",
    "#Used to generate a random value in the mutation operation\n",
    "class feature_range:\n",
    "    def __init__(self, column, col_type, min_value, max_value):\n",
    "        self.column = column \n",
    "        self.col_type = col_type\n",
    "        self.min_value = min_value\n",
    "        self.max_value = max_value\n",
    "\n",
    "    #Returns a random value to perform mutation operation\n",
    "    def get_random_value(self):\n",
    "        if self.col_type == 'int64' or self.col_type == 'int' or self.col_type == 'int16' or self.col_type == 'int8' or (self.col_type == 'uint8'):\n",
    "            value = rnd.randint(self.min_value, self.max_value)\n",
    "        else:  \n",
    "            value = round(rnd.uniform(self.min_value, self.max_value), 2)\n",
    "        return value\n",
    "    \n",
    "    #Checks if the attribute has only one value.\n",
    "    def unique_value(self):\n",
    "        if self.min_value != self.max_value:\n",
    "            return False\n",
    "        else:  \n",
    "            return True    \n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr((self.column, self.col_type, self.min_value, self.max_value)) \n",
    "        \n",
    "class CSSE(object):\n",
    "    \n",
    "    def __init__(self, input_dataset, model, static_list = [], K = 3, num_gen = 30, pop_size = 100, per_elit = 0.1, cros_proba = 0.8, mutation_proba = 0.1, L1 = 1, L2 = 1):\n",
    "        #User Options\n",
    "        self.static_list = static_list #List of static features\n",
    "        self.K = K #Number of counterfactuals desired\n",
    "        #Model\n",
    "        self.input_dataset = input_dataset\n",
    "        self.model = model\n",
    "        #GA Parameters\n",
    "        self.num_gen = num_gen\n",
    "        self.pop_size = pop_size\n",
    "        self.per_elit = per_elit\n",
    "        self.cros_proba = cros_proba\n",
    "        self.mutation_proba = mutation_proba\n",
    "        #Objective function parameters\n",
    "        self.L1 = L1 #weight assigned the distance to the original instance\n",
    "        self.L2 = L2 #weight assigned the number of changes needed in the original instance   \n",
    "    \n",
    "    #Get which index in the SHAP corresponding to the current class\n",
    "    def getBadClass(self):   \n",
    "        if self.current_class == self.model.classes_[0]:\n",
    "            ind_cur_class = 0\n",
    "        else:\n",
    "            ind_cur_class = 1\n",
    "        \n",
    "        return ind_cur_class\n",
    "    \n",
    "    #Gets the valid values range for each feature\n",
    "    def getFeaturesRange(self):\n",
    "        features_range = []\n",
    "       \n",
    "        for i in range (0, self.input_dataset.columns.size):\n",
    "            col_name = self.input_dataset.columns[i]\n",
    "            col_type = self.input_dataset[col_name].dtype\n",
    "            min_value = min(self.input_dataset[col_name])\n",
    "            max_value = max(self.input_dataset[col_name])\n",
    "            \n",
    "            feature_range_ind = feature_range(col_name, col_type, min_value, max_value)\n",
    "            features_range.append(feature_range_ind)\n",
    "        \n",
    "        return features_range\n",
    "       \n",
    "    def getMutationValue(self, currentValue, index, ind_feature_range):\n",
    "        new_value = ind_feature_range.get_random_value()\n",
    "        \n",
    "        while currentValue == new_value:\n",
    "            new_value = ind_feature_range.get_random_value()\n",
    "        \n",
    "        return new_value\n",
    "    \n",
    "    def equal(self, individual, population):\n",
    "        aux = 0\n",
    "        for i in range ( 1, len(population)):\n",
    "            c = population.loc[i].copy()\n",
    "            dst = distance.euclidean(individual, c)\n",
    "            if dst == 0:\n",
    "                aux = 1\n",
    "        \n",
    "        return aux\n",
    "\n",
    "    def getPopInicial (self, df, features_range): \n",
    "        #The reference individual will always be in the 0 position of the df - so that it is normalized as well (it will be used later in the distance function)\n",
    "        df.loc[0] = self.original_ind.copy()\n",
    "        \n",
    "        #Counting numbers of repeated individuals\n",
    "        number_repetitions = 0\n",
    "        \n",
    "        #One more position is used because the zero position was reserved for the reference individual\n",
    "        while len(df) < self.pop_size + 1:\n",
    "            #Draw a feature to change\n",
    "            index_a = rnd.randint( 0, self.input_dataset.columns.size - 1 )\n",
    "            while df.columns[index_a] in self.static_list:\n",
    "                index_a = rnd.randint( 0, self.input_dataset.columns.size - 1 )\n",
    "                \n",
    "            if not features_range[index_a].unique_value():\n",
    "                #Mutation\n",
    "                mutant = self.original_ind.copy()\n",
    "\n",
    "                new_value =  self.getMutationValue(mutant.iloc[index_a], index_a, features_range[index_a])\n",
    "                mutant.iloc[index_a] = new_value\n",
    "\n",
    "                ni = self.equal(mutant, df)\n",
    "                if ni == 0:\n",
    "                    df.loc[len(df)] = mutant.copy()\n",
    "                else:\n",
    "                    #Assesses whether the GA is producing too many repeated individuals.\n",
    "                    number_repetitions = number_repetitions + 1\n",
    "                    if number_repetitions == 2*self.pop_size:\n",
    "                        self.pop_size = round(self.pop_size - self.pop_size*0.1)\n",
    "                        self.mutation_proba = self.mutation_proba + 0.1\n",
    "                        #print('Adjusting population size...', self.pop_size)\n",
    "                        number_repetitions = 0\n",
    "    \n",
    "    #Complete the standardized proximity and similarity assessments for each individual\n",
    "    def getNormalEvaluation(self, evaluation, aval_norma):\n",
    "        scaler2 = preprocessing.MinMaxScaler()\n",
    "        aval_norma2 = scaler2.fit_transform(aval_norma)\n",
    "    \n",
    "        i = 0\n",
    "        while i < len(evaluation):\n",
    "            evaluation[i].aval_norm = self.L1*aval_norma2[i,0] + self.L2*aval_norma2[i,1] + aval_norma2[i,2]\n",
    "            evaluation[i].dist_norm = self.L1*aval_norma2[i,0] + self.L2*aval_norma2[i,1]\n",
    "        \n",
    "            i = i + 1\n",
    "    \n",
    "    def numChanges(self, ind_con):\n",
    "        num = 0\n",
    "        for i in range(len(self.original_ind)):\n",
    "            if self.original_ind[i] != ind_con[i]:\n",
    "                num = num + 1\n",
    "        \n",
    "        return num\n",
    "        \n",
    "    def fitness(self, population, evaluation, ind_cur_class):\n",
    "        def getProximityEvaluation (proba):\n",
    "            #Penalizes the individual who is in the negative class\n",
    "            if proba < 0.5:\n",
    "                predict_score = 0\n",
    "            else:\n",
    "                predict_score= proba\n",
    "             \n",
    "            return predict_score\n",
    "               \n",
    "        #Calculates similarity to the original instance\n",
    "        def getEvaluationDist (ind, X_train_minmax):\n",
    "            #Normalizes the data so that the different scales do not bias the distance\n",
    "            a = X_train_minmax[0]\n",
    "            b = X_train_minmax[ind]\n",
    "            dst = distance.euclidean(a, b)\n",
    "  \n",
    "            return dst\n",
    "        \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "        \n",
    "            predict_proba = self.model.predict_proba(population)\n",
    "                    \n",
    "        #Calculating the distance between instances\n",
    "        scaler = preprocessing.MinMaxScaler()\n",
    "        X_train_minmax = scaler.fit_transform(population)\n",
    "    \n",
    "        i = 0\n",
    "        aval_norma = [] \n",
    "        while i < len(population):\n",
    "            proximityEvaluation = getProximityEvaluation(predict_proba[i, ind_cur_class])\n",
    "            evaldist = getEvaluationDist(i, X_train_minmax)\n",
    "            #The original individual is in the 1st position\n",
    "            numChanges = self.numChanges(population.loc[i])\n",
    "        \n",
    "            ind = individual(i, proximityEvaluation, evaldist, numChanges, 0, 0, predict_proba[i, ind_cur_class])\n",
    "            aval_norma.append([evaldist, numChanges, proximityEvaluation])\n",
    "            evaluation.append(ind)\n",
    "            i = i + 1\n",
    "\n",
    "        self.getNormalEvaluation(evaluation, aval_norma)\n",
    "       \n",
    "    #Given a counterfactual solution returns the list of modified columns\n",
    "    def getColumns(self, counter_solution):\n",
    "        colums = []\n",
    "        for j in range (0, len(counter_solution)):\n",
    "            colums.append(counter_solution[j].column)\n",
    "        \n",
    "        return colums      \n",
    "             \n",
    "    #Checks if the new solution is contained in the solutions already found\n",
    "    def contained_solution(self, original_instance, current_list, current_column_list, new_solution, new_column_solution):\n",
    "        contained = False\n",
    "        for i in range (0, len(current_list)):              \n",
    "            if set(current_column_list[i]).issubset(new_column_solution):\n",
    "                for j in range (0, len(current_list[i])):\n",
    "                    pos = new_column_solution.index(current_list[i][j].column)\n",
    "                    distancia_a = abs(original_instance[current_list[i][j].column] - current_list[i][j].value)\n",
    "                    distancia_b = abs(original_instance[current_list[i][j].column] - new_solution[pos].value)\n",
    "                    if distancia_b >= distancia_a:\n",
    "                        contained = True\n",
    "\n",
    "        return contained\n",
    "      \n",
    "    def elitism(self, evaluation, df, parents):\n",
    "         \n",
    "        num_elit = round(self.per_elit*self.pop_size)\n",
    "        \n",
    "        aval = []\n",
    "        aval = evaluation.copy()\n",
    "        aval.sort(key=lambda individual: individual.aval_norm)\n",
    "        \n",
    "        #contrafactual_ind = pd.DataFrame(columns=self.input_dataset.columns)\n",
    "        solution_list = []\n",
    "        solution_colums_list = []\n",
    "        \n",
    "        i = 0\n",
    "        numContraf = 0\n",
    "        while i < len(aval) and numContraf <= num_elit + 1:\n",
    "            #Checks if the example belongs to the counterfactual class\n",
    "            if aval[i].predict_proba < 0.5:\n",
    "                ind_changes = []\n",
    "                ind_colums_change = []\n",
    "         \n",
    "                #Gets counterfactual example change list\n",
    "                ind_changes = self.getChanges(aval[i].index, parents)\n",
    "                #Generates the list of columns modified in the counterfactual to check if there is already a solution with that set of columns\n",
    "                ind_colums_change = self.getColumns(ind_changes)\n",
    "                \n",
    "                if ind_colums_change not in solution_colums_list:\n",
    "                    #Check if one solution is a subset of the other\n",
    "                    if not self.contained_solution(self.original_ind, solution_list, solution_colums_list, ind_changes, ind_colums_change):\n",
    "                        #Include counterfactual in the list of examples of the final solution                    \n",
    "                        df.loc[len(df)] = parents.iloc[aval[i].index].copy()                     \n",
    "                                \n",
    "                        #Add to the list of solutions (changes only)       \n",
    "                        solution_list.append(ind_changes)\n",
    "                        #Used to compare with the next counterfactuals (to ensure diversity)\n",
    "                        solution_colums_list.append(ind_colums_change)\n",
    "                                        \n",
    "                        numContraf = numContraf + 1\n",
    "                      \n",
    "            i = i + 1\n",
    "        return solution_list\n",
    "    \n",
    "    def roulette_wheel(self, evaluation):\n",
    "        summation = 0\n",
    "        #Performs roulette wheel to select parents who will undergo genetic operations\n",
    "        for i in range (1, len(evaluation)): \n",
    "            summation = summation + 1/evaluation[i].aval_norm\n",
    "    \n",
    "        roulette = rnd.uniform( 0, summation )\n",
    "    \n",
    "        roulette_score = 1/evaluation[1].aval_norm\n",
    "        i = 1\n",
    "        while roulette_score < roulette:\n",
    "            i += 1\n",
    "            roulette_score += 1/evaluation[i].aval_norm\n",
    "        \n",
    "        return i\n",
    "            \n",
    "    def crossover (self, df, parents, evaluation, number_cross_repetitions):\n",
    "        child = []\n",
    "            \n",
    "        corte = rnd.randint( 0, self.input_dataset.columns.size - 1 )\n",
    "            \n",
    "        index1 = self.roulette_wheel(evaluation)\n",
    "        index2 = self.roulette_wheel(evaluation)\n",
    "        \n",
    "        ind_a = parents.iloc[index1].copy()\n",
    "        ind_b = parents.iloc[index2].copy()\n",
    "            \n",
    "        crossover_op = rnd.random()\n",
    "        if crossover_op <= self.cros_proba:\n",
    "            child[ :corte ] = ind_a[ :corte ].copy()\n",
    "            child[ corte: ] = ind_b[ corte: ].copy()\n",
    "        else:\n",
    "            child = ind_a.copy()\n",
    "        \n",
    "        ni = self.equal(child, df)\n",
    "        if ni == 0:\n",
    "            df.loc[len(df)] = child.copy()\n",
    "        else:\n",
    "            #Assesses whether the GA is producing too many repeated individuals.\n",
    "            number_cross_repetitions = number_cross_repetitions + 1\n",
    "            if number_cross_repetitions == self.pop_size:\n",
    "                self.pop_size = round(self.pop_size - self.pop_size*0.1)\n",
    "                self.mutation_proba = self.mutation_proba + 0.1\n",
    "                #print('Adjusting population size...', self.pop_size)\n",
    "                number_cross_repetitions = 0\n",
    "        #    print('repeated')\n",
    "        return number_cross_repetitions\n",
    "                       \n",
    "    def mutation (self, df, individual_pos, features_range):\n",
    "        ni = 1\n",
    "        #Does not allow repeated individual\n",
    "        while ni == 1:\n",
    "            #Draw a feature to change\n",
    "            index_a = rnd.randint( 0, self.input_dataset.columns.size - 1 )\n",
    "            while df.columns[index_a] in self.static_list:\n",
    "                index_a = rnd.randint( 0, self.input_dataset.columns.size - 1 )\n",
    "            \n",
    "            if not features_range[index_a].unique_value():\n",
    "                #Mutation\n",
    "                mutant = df.iloc[individual_pos].copy()\n",
    "            \n",
    "                #Draw the value to be changed\n",
    "                new_value =  self.getMutationValue(mutant.iloc[index_a], index_a, features_range[index_a])  \n",
    "                mutant.iloc[index_a] = new_value\n",
    "\n",
    "                ni = self.equal(mutant, df)\n",
    "                if ni == 0:\n",
    "                    df.loc[individual_pos] = mutant.copy()\n",
    "                #else:\n",
    "                #    print('repeated')\n",
    "     \n",
    "    def getChanges(self, ind, dfComp):\n",
    "        changes = []\n",
    "        \n",
    "        for i in range (len(dfComp.iloc[ind])):\n",
    "            if self.original_ind[i] != dfComp.loc[ind][i]:\n",
    "                counter_change_ind = counter_change(dfComp.columns[i], dfComp.loc[ind][i])\n",
    "                changes.append(counter_change_ind)\n",
    "\n",
    "        return changes\n",
    "    \n",
    "    #Generates the solution from the final population\n",
    "    def getContrafactual(self, df, aval):\n",
    "        \n",
    "        contrafactual_ind = pd.DataFrame(columns=self.input_dataset.columns)\n",
    "        solution_list = []\n",
    "        solution_colums_list = []\n",
    "        \n",
    "        i = 0\n",
    "        numContraf = 0\n",
    "        while i < len(aval) and numContraf < self.K:\n",
    "            #Checks if the example belongs to the counterfactual class\n",
    "            if aval[i].predict_proba < 0.5:\n",
    "                ind_changes = []\n",
    "                ind_colums_change = []\n",
    "         \n",
    "                #Gets counterfactual example change list\n",
    "                ind_changes = self.getChanges(aval[i].index, df)\n",
    "                #Generates the list of columns modified in the counterfactual to check if there is already a solution with that set of columns\n",
    "                ind_colums_change = self.getColumns(ind_changes)\n",
    "                \n",
    "                if ind_colums_change not in solution_colums_list:\n",
    "                    #Check if one solution is a subset of the other\n",
    "                    if not self.contained_solution(self.original_ind, solution_list, solution_colums_list, ind_changes, ind_colums_change):\n",
    "                        #Include counterfactual in the list of examples of the final solution\n",
    "                        contrafactual_ind.loc[len(contrafactual_ind)] = df.iloc[aval[i].index].copy()\n",
    "                                \n",
    "                        #Add to the list of solutions (changes only)       \n",
    "                        solution_list.append(ind_changes)\n",
    "                        #Used to compare with the next counterfactuals (to ensure diversity)\n",
    "                        solution_colums_list.append(ind_colums_change)\n",
    "                                        \n",
    "                        numContraf = numContraf + 1\n",
    "                        #print('solution_list ', solution_list)\n",
    "                    #else:\n",
    "                        #print('is contained ', ind_changes)\n",
    "                #else:\n",
    "                    #print('repeated ', ind_changes)\n",
    "                      \n",
    "            i = i + 1\n",
    "\n",
    "        return contrafactual_ind, solution_list   \n",
    "    \n",
    "    def printResults(self, solution):\n",
    "        print(\"Result obtained\")\n",
    "        if len(solution) != 0:\n",
    "            for i in range(0, len(solution)): \n",
    "                print(\"\\n\")\n",
    "                print(f\"{'Counterfactual ' + str(i + 1):^34}\")\n",
    "                for j in range(0, len(solution[i])): \n",
    "                    print(f\"{str(solution[i][j].column):<29} {str(solution[i][j].value):>5}\")\n",
    "        else:\n",
    "            print('Solution not found. It may be necessary to adjust the parameters for this instance.')\n",
    "                                                 \n",
    "    def explain(self, original_ind, current_class):\n",
    "        self.original_ind = original_ind #Original instance\n",
    "        #self.ind_cur_class = ind_cur_class #Index in the shap corresponds to the original instance class\n",
    "        self.current_class = current_class #Original instance class\n",
    "        \n",
    "        ind_cur_class = self.getBadClass()\n",
    "    \n",
    "        #Gets the valid values range of each feature\n",
    "        features_range = []\n",
    "        features_range = self.getFeaturesRange()\n",
    "\n",
    "        #The DataFrame df will have the current population\n",
    "        df = pd.DataFrame(columns=self.input_dataset.columns)\n",
    "        \n",
    "        #Generates the initial population with popinitial mutants        \n",
    "        self.getPopInicial(df, features_range)\n",
    "        for g in range(self.num_gen):\n",
    "            #To use on the parents of each generation\n",
    "            parents = pd.DataFrame(columns=self.input_dataset.columns)\n",
    "    \n",
    "            #Copy parents to the next generation\n",
    "            parents = df.copy()\n",
    "            #df will contain the new population\n",
    "            df = pd.DataFrame(columns=self.input_dataset.columns)\n",
    "            \n",
    "            evaluation = []                         \n",
    "                   \n",
    "            #Assessing generation counterfactuals\n",
    "            self.fitness(parents, evaluation, ind_cur_class)\n",
    "            #The original individual will always be in the 0 position of the df - So that it is normalized too (it will be used later in the distance function)\n",
    "            df.loc[0] = self.original_ind.copy()\n",
    "            \n",
    "            #Copies to the next generation the per_elit best individuals\n",
    "            self.elitism(evaluation, df, parents)\n",
    "            number_cross_repetitions = 0\n",
    "            while len(df) < self.pop_size + 1: #+1, as the 1st position is used to store the reference individual\n",
    "                number_cross_repetitions = self.crossover(df, parents, evaluation, number_cross_repetitions)\n",
    "                \n",
    "                mutation_op = rnd.random()\n",
    "                if mutation_op <= self.mutation_proba:\n",
    "                    self.mutation(df, len(df) - 1, features_range)\n",
    "            \n",
    "            print()\n",
    "                 \n",
    "        evaluation = []\n",
    "    \n",
    "        #Evaluating the latest generation\n",
    "        self.fitness(df, evaluation, ind_cur_class)\n",
    "    \n",
    "        #Order the last generation by distance to the original instance     \n",
    "        evaluation.sort(key=lambda individual: individual.aval_norm)     \n",
    "        \n",
    "        #Getting the counterfactual set\n",
    "        contrafactual_set = pd.DataFrame(columns=self.input_dataset.columns)\n",
    "        contrafactual_set, solution_list = self.getContrafactual(df, evaluation)       \n",
    "                 \n",
    "        return contrafactual_set, solution_list\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CCSSE:\n",
    "    def __init__(self, dataset, bb_model, samples = None, K = 5, generation = 10):\n",
    "        self.df_datasets = self.load_df_dataset()\n",
    "        self.dataset = dataset\n",
    "        self.samples = samples\n",
    "        self.K = K\n",
    "        self.generation = generation\n",
    "        \n",
    "#         self.x_train, self.x_test, self.y_train, self.y_test, self.dfx_full, self.dfy_full = self.get_datasets_train_test()\n",
    "        self.x_train, self.x_test, self.y_train, self.y_test, self.dfx_full, self.dfy_full = self.get_dataset()\n",
    "\n",
    "        self.bb_model, self.p = self.get_bb_model(bb_model)\n",
    "        self.explainerCSSE = self.get_model_contrafactual()\n",
    "        self.model_causal, self.df_causal_effects, self.df_error, self.causal_order = self.get_model_causality()\n",
    "    \n",
    "        self.run_dict = {}\n",
    "        self.run_non_causal_dict = {}\n",
    "        \n",
    "    def load_df_dataset(self):\n",
    "        def convert_to_list(val):\n",
    "            try:\n",
    "                return ast.literal_eval(val) if isinstance(val, str) and val.startswith('[') and val.endswith(']') else val\n",
    "            except (ValueError, SyntaxError):\n",
    "                return val\n",
    "            \n",
    "        df = pd.read_parquet(\"s3://omar-testes-gerais/artigos/artifacts/df_map_inference_datasets.parquet\")\n",
    "        df['path'] = df['path'].apply(convert_to_list)\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n",
    "    def get_dataset(self):\n",
    "        dataset_dict = self.df_datasets[self.df_datasets['name'] == self.dataset].iloc[0].to_dict()\n",
    "        \n",
    "        if isinstance(dataset_dict['path'], list):\n",
    "            if 'Column' in dataset_dict['classe']:\n",
    "                df_train = pd.read_csv(f\"s3://omar-testes-gerais/artigos/artifacts/datasets/{dataset_dict['path'][0]}\", header = None)\n",
    "                df_test = pd.read_csv(f\"s3://omar-testes-gerais/artigos/artifacts/datasets/{dataset_dict['path'][1]}\", header = None)\n",
    "                class_name = int(dataset_dict['class'].split('Column')[1]) - 1\n",
    "            else:\n",
    "                df_train = pd.read_csv(f\"s3://omar-testes-gerais/artigos/artifacts/datasets/{dataset_dict['path'][0]}\")\n",
    "                df_test = pd.read_csv(f\"s3://omar-testes-gerais/artigos/artifacts/datasets/{dataset_dict['path'][1]}\")\n",
    "                class_name = dataset_dict['classe']\n",
    "            \n",
    "            x_train = df_train.drop(columns=[class_name])\n",
    "            y_train = df_train[class_name]\n",
    "\n",
    "            # Dividindo o df_test\n",
    "            x_test = df_test.drop(columns=[class_name])\n",
    "            y_test = df_test[class_name]\n",
    "            \n",
    "            dfx_full = pd.concat([x_train, x_test])\n",
    "            dfy_full = pd.concat([y_train, y_test])\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            if 'Column' in dataset_dict['classe']:\n",
    "                df_main = pd.read_csv(f\"s3://omar-testes-gerais/artigos/artifacts/datasets/{dataset_dict['path']}\", header = None)\n",
    "                class_name = int(dataset_dict['classe'].split('Column')[1]) - 1\n",
    "            else:\n",
    "                df_main = pd.read_csv(f\"s3://omar-testes-gerais/artigos/artifacts/datasets/{dataset_dict['path']}\")\n",
    "                class_name = dataset_dict['classe']\n",
    "                \n",
    "            columns = df_main.columns\n",
    "            columns_tmp = list(columns)\n",
    "            columns_tmp.remove(class_name)\n",
    "\n",
    "            x_train, x_test, y_train, y_test = train_test_split(df_main[columns_tmp], df_main[class_name], test_size=0.1)\n",
    "\n",
    "            dfx_full = pd.concat([x_train, x_test])\n",
    "            dfy_full = pd.concat([y_train, y_test])\n",
    "            \n",
    "        return x_train, x_test, y_train, y_test, dfx_full, dfy_full\n",
    "    \n",
    "    def get_datasets_train_test(self):\n",
    "        if self.dataset == \"german_short\":\n",
    "            map_columns = {\n",
    "                'Unnamed: 0': 'index',\n",
    "                'x0': 'Sex',\n",
    "                'x1': 'Age',\n",
    "                'x2': 'Credit',\n",
    "                'x3': 'LoanDuration',\n",
    "            }\n",
    "            x_train = pd.read_csv(\"data/algrec_german/X_train_short.csv\").rename(columns = map_columns)\n",
    "            x_test = pd.read_csv(\"data/algrec_german/X_test_short.csv\").rename(columns = map_columns)\n",
    "            y_train = pd.read_csv(\"data/algrec_german/y_train_short.csv\").rename(columns={'Unnamed: 0': 'index'})\n",
    "            y_test = pd.read_csv(\"data/algrec_german/y_test_short.csv\").rename(columns={'Unnamed: 0': 'index'})\n",
    "            x_train = x_train.set_index('index')\n",
    "            x_test = x_test.set_index('index')\n",
    "            y_train = y_train.set_index('index')\n",
    "            y_test = y_test.set_index('index')\n",
    "            dfx_full = pd.concat([x_train, x_test])\n",
    "            dfy_full = pd.concat([y_train, y_test])\n",
    "\n",
    "        elif self.dataset == \"german_medium\":\n",
    "            map_columns = {\n",
    "                'Unnamed: 0': 'index',\n",
    "                'x0': 'Sex',\n",
    "                'x1': 'Age',\n",
    "                'x2': 'Credit',\n",
    "                'x3': 'LoanDuration',\n",
    "                'x4': 'CheckingAccountBalance',\n",
    "                'x5':'SavingsAccountBalance',\n",
    "                'x6':'HousingStatus'\n",
    "            }\n",
    "            x_train = pd.read_csv(\"data/algrec_german/X_train.csv\").rename(columns = map_columns)\n",
    "            x_test = pd.read_csv(\"data/algrec_german/X_test.csv\").rename(columns = map_columns)\n",
    "            y_train = pd.read_csv(\"data/algrec_german/y_train.csv\").rename(columns={'Unnamed: 0': 'index'})\n",
    "            y_test = pd.read_csv(\"data/algrec_german/y_test.csv\").rename(columns={'Unnamed: 0': 'index'})\n",
    "            x_train = x_train.set_index('index')\n",
    "            x_test = x_test.set_index('index')\n",
    "            y_train = y_train.set_index('index')\n",
    "            y_test = y_test.set_index('index')\n",
    "            dfx_full = pd.concat([x_train, x_test])\n",
    "            dfy_full = pd.concat([y_train, y_test])\n",
    "\n",
    "        elif self.dataset == \"german_full\":\n",
    "            df_main = prepare_german_dataset(\"german_credit.csv\", \"data/\")\n",
    "            columns = df_main.columns\n",
    "            class_name = 'default' # default = 0 = \"Good class\" / default = 1 = \"Bad class\" \n",
    "            columns_tmp = list(columns)\n",
    "            columns_tmp.remove(class_name)\n",
    "\n",
    "            x_train, x_test, y_train, y_test = train_test_split(df_main[columns_tmp], df_main[class_name], test_size=0.1)\n",
    "\n",
    "            dfx_full = pd.concat([x_train, x_test])\n",
    "            dfy_full = pd.concat([y_train, y_test])\n",
    "            \n",
    "        elif self.dataset == 'np':\n",
    "            df_main = pd.read_csv(\"data/breast_coimbra_np.csv\")\n",
    "            columns = df_main.columns\n",
    "            class_name = 'Classification' # default = 0 = \"Good class\" / default = 1 = \"Bad class\" \n",
    "            columns_tmp = list(columns)\n",
    "            columns_tmp.remove(class_name)\n",
    "\n",
    "            x_train, x_test, y_train, y_test = train_test_split(df_main[columns_tmp], df_main[class_name], test_size=0.1)\n",
    "\n",
    "            dfx_full = pd.concat([x_train, x_test])\n",
    "            dfy_full = pd.concat([y_train, y_test])\n",
    "            \n",
    "        elif self.dataset == 'nm':\n",
    "            df_main = pd.read_csv(\"data/heloc_dataset_v1_nm.csv\")\n",
    "            columns = df_main.columns\n",
    "            class_name = 'RiskPerformance' # default = 0 = \"Good class\" / default = 1 = \"Bad class\" \n",
    "            columns_tmp = list(columns)\n",
    "            columns_tmp.remove(class_name)\n",
    "\n",
    "            x_train, x_test, y_train, y_test = train_test_split(df_main[columns_tmp], df_main[class_name], test_size=0.1)\n",
    "\n",
    "            dfx_full = pd.concat([x_train, x_test])\n",
    "            dfy_full = pd.concat([y_train, y_test])\n",
    "            \n",
    "        elif self.dataset == 'cm':\n",
    "            df_main = pd.read_csv(\"data/house_votes_84_processada_cm.csv\")\n",
    "            columns = df_main.columns\n",
    "            class_name = 'Class Name' # default = 0 = \"Good class\" / default = 1 = \"Bad class\" \n",
    "            columns_tmp = list(columns)\n",
    "            columns_tmp.remove(class_name)\n",
    "\n",
    "            x_train, x_test, y_train, y_test = train_test_split(df_main[columns_tmp], df_main[class_name], test_size=0.1)\n",
    "\n",
    "            dfx_full = pd.concat([x_train, x_test])\n",
    "            dfy_full = pd.concat([y_train, y_test])\n",
    "            \n",
    "        elif self.dataset == 'ng':\n",
    "            df_main = pd.read_csv(\"data/ionosphere_ng.csv\")\n",
    "            columns = df_main.columns\n",
    "            class_name = 'target' # default = 0 = \"Good class\" / default = 1 = \"Bad class\" \n",
    "            columns_tmp = list(columns)\n",
    "            columns_tmp.remove(class_name)\n",
    "\n",
    "            x_train, x_test, y_train, y_test = train_test_split(df_main[columns_tmp], df_main[class_name], test_size=0.1)\n",
    "\n",
    "            dfx_full = pd.concat([x_train, x_test])\n",
    "            dfy_full = pd.concat([y_train, y_test])\n",
    "            \n",
    "        elif self.dataset == 'tokyo_ng':\n",
    "            df_main = pd.read_csv(\"data/Tokyo_ng.csv\").rename(columns = {'Unnamed: 44': 'class'})\n",
    "            columns = df_main.columns\n",
    "            class_name = 'class' # default = 0 = \"Good class\" / default = 1 = \"Bad class\" \n",
    "            columns_tmp = list(columns)\n",
    "            columns_tmp.remove(class_name)\n",
    "\n",
    "            x_train, x_test, y_train, y_test = train_test_split(df_main[columns_tmp], df_main[class_name], test_size=0.1)\n",
    "\n",
    "            dfx_full = pd.concat([x_train, x_test])\n",
    "            dfy_full = pd.concat([y_train, y_test])\n",
    "            \n",
    "        elif self.dataset == 'breast-cancer_ng':\n",
    "            df_main = pd.read_csv(\"data/breast-cancer_ng.csv\")\n",
    "            columns = df_main.columns\n",
    "            class_name = 'diagnosis' # default = 0 = \"Good class\" / default = 1 = \"Bad class\" \n",
    "            columns_tmp = list(columns)\n",
    "            columns_tmp.remove(class_name)\n",
    "\n",
    "            x_train, x_test, y_train, y_test = train_test_split(df_main[columns_tmp], df_main[class_name], test_size=0.1)\n",
    "\n",
    "            dfx_full = pd.concat([x_train, x_test])\n",
    "            dfy_full = pd.concat([y_train, y_test])\n",
    "        \n",
    "        elif self.dataset == 'cp':\n",
    "            df_train = pd.read_csv(\"data/monks-1_train_cp.csv\")\n",
    "\n",
    "            #Get the input features\n",
    "            columns = df_train.columns\n",
    "            class_name = 'Class' # default = 0 = \"Good class\" / default = 1 = \"Bad class\" \n",
    "            columns_tmp = list(columns)\n",
    "            columns_tmp.remove(class_name)\n",
    "            columns_tmp.remove('Id')\n",
    "\n",
    "            x_train = df_train[columns_tmp]\n",
    "            y_train = df_train[['Class']]\n",
    "\n",
    "            # x_train, x_test, y_train, y_test = train_test_split(df_main[columns_tmp], df_main[class_name], test_size=0.1)\n",
    "\n",
    "            model = RandomForestClassifier()  \n",
    "            model.fit(x_train, y_train)\n",
    "\n",
    "            df_test = pd.read_csv('data/monks-1_test_cp.csv')\n",
    "\n",
    "            #Get the input features\n",
    "            columns = df_test.columns\n",
    "            class_name = 'Class' # default = 0 = \"Good class\" / default = 1 = \"Bad class\" \n",
    "            columns_tmp = list(columns)\n",
    "            columns_tmp.remove(class_name)\n",
    "            columns_tmp.remove('Id')\n",
    "\n",
    "            x_test = df_test[columns_tmp]\n",
    "            y_test = df_test[['Class']]\n",
    "\n",
    "            dfx_full = pd.concat([x_train, x_test])\n",
    "            dfy_full = pd.concat([y_train, y_test])\n",
    "            \n",
    "        elif self.dataset == 'cg':\n",
    "            df_main = pd.read_csv(\"data/mushrooms_processada_cg.csv\")[:1000]\n",
    "            columns = df_main.columns\n",
    "            class_name = 'class' # default = 0 = \"Good class\" / default = 1 = \"Bad class\" \n",
    "            columns_tmp = list(columns)\n",
    "            columns_tmp.remove(class_name)\n",
    "\n",
    "            x_train, x_test, y_train, y_test = train_test_split(df_main[columns_tmp], df_main[class_name], test_size=0.1)\n",
    "\n",
    "            dfx_full = pd.concat([x_train, x_test])\n",
    "            dfy_full = pd.concat([y_train, y_test])\n",
    "            \n",
    "        elif self.dataset == 'Phishing_cg':\n",
    "            df_main = pd.read_csv(\"data/Phishing_cg.csv\")[2:1000].astype(float)\n",
    "            columns = df_main.columns\n",
    "            class_name = 'Class' # default = 0 = \"Good class\" / default = 1 = \"Bad class\" \n",
    "            columns_tmp = list(columns)\n",
    "            columns_tmp.remove(class_name)\n",
    "\n",
    "            x_train, x_test, y_train, y_test = train_test_split(df_main[columns_tmp], df_main[class_name], test_size=0.1)\n",
    "\n",
    "            dfx_full = pd.concat([x_train, x_test])\n",
    "            dfy_full = pd.concat([y_train, y_test])\n",
    "\n",
    "        else:\n",
    "            x_train = pd.DataFrame()\n",
    "            x_test = pd.DataFrame()\n",
    "            y_train = pd.DataFrame()\n",
    "            y_test = pd.DataFrame()\n",
    "            dfx_full = pd.DataFrame()\n",
    "            dfy_full = pd.DataFrame()\n",
    "\n",
    "        return x_train, x_test, y_train, y_test, dfx_full, dfy_full\n",
    "    \n",
    "\n",
    "    def get_bb_model(self, bb_model_name):\n",
    "        \n",
    "        if bb_model_name == 'rf':\n",
    "            bb_model = RandomForestClassifier()  \n",
    "            bb_model.fit(self.x_train, self.y_train)\n",
    "\n",
    "            p = bb_model.predict(self.x_test)\n",
    "\n",
    "            print(classification_report(self.y_test, p))\n",
    "\n",
    "            return bb_model, p\n",
    "        elif bb_model_name == 'rn':\n",
    "            pass\n",
    "\n",
    "    def get_model_contrafactual(self):\n",
    "        return CSSE(self.dfx_full, self.bb_model, K = self.K, num_gen = self.generation)\n",
    "\n",
    "    def get_model_causality(self):\n",
    "        model_causal = lingam.DirectLiNGAM()\n",
    "        model_causal.fit(self.dfx_full)\n",
    "            \n",
    "        labels = [f'{i}' for i in self.dfx_full.columns]\n",
    "        causal_order = [labels[x] for x in model_causal.causal_order_]\n",
    "        \n",
    "        matrix = model_causal.adjacency_matrix_\n",
    "        from_list = []\n",
    "        to_list = []\n",
    "        effect_list = []\n",
    "\n",
    "        # Iteração sobre a matriz para extrair os valores e suas posições\n",
    "        for i in range(len(matrix)):\n",
    "            for j in range(len(matrix[i])):\n",
    "                if matrix[i][j] != 0:\n",
    "                    from_list.append(j)\n",
    "                    to_list.append(i)\n",
    "                    effect_list.append(matrix[i][j])\n",
    "\n",
    "        # Criando o DataFrame\n",
    "        df_causal_effects = pd.DataFrame({'from': from_list, 'to': to_list, 'effect': effect_list})\n",
    "        labels = [f'{i}' for i in self.dfx_full.columns]\n",
    "        df_causal_effects['from'] = df_causal_effects['from'].apply(lambda x : labels[x])\n",
    "        df_causal_effects['to'] = df_causal_effects['to'].apply(lambda x : labels[x])\n",
    "\n",
    "\n",
    "        matrix_error = model_causal.get_error_independence_p_values(self.dfx_full)\n",
    "        from_list = []\n",
    "        to_list = []\n",
    "        effect_list = []\n",
    "\n",
    "        # Iteração sobre a matriz para extrair os valores e suas posições\n",
    "        for i in range(len(matrix_error)):\n",
    "            for j in range(i + 1, len(matrix_error[i])):\n",
    "                if matrix_error[i][j] != 0:\n",
    "                    from_list.append(j)\n",
    "                    to_list.append(i)\n",
    "                    effect_list.append(matrix_error[i][j])\n",
    "\n",
    "        # Criando o DataFrame\n",
    "        df_error = pd.DataFrame({'from': from_list, 'to': to_list, 'effect': effect_list})\n",
    "        labels = [f'{i}' for i in self.dfx_full.columns]\n",
    "        df_error['from'] = df_error['from'].apply(lambda x : labels[x])\n",
    "        df_error['to'] = df_error['to'].apply(lambda x : labels[x])\n",
    "        \n",
    "\n",
    "        return model_causal, df_causal_effects, df_error, causal_order\n",
    "        \n",
    "    \n",
    "    def print_causal_graph(self):\n",
    "        make_dot(self.model_causal.adjacency_matrix_)\n",
    "\n",
    "    def run_non_causal(self):\n",
    "        self.run_non_causal_dict = {}\n",
    "\n",
    "        if isinstance(self.samples, list):\n",
    "            self.create_run_dict(self)\n",
    "            for sample in self.samples:\n",
    "                self.run_non_causal_sample(sample)\n",
    "                \n",
    "        elif isinstance(self.samples, int):\n",
    "            for sample in range(self.samples):\n",
    "                self.run_non_causal_sample(sample)\n",
    "        \n",
    "        else: \n",
    "            for sample in range(10):\n",
    "                self.run_non_causal_sample(sample)\n",
    "                \n",
    "    def run_non_causal_sample(self, sample):\n",
    "        self.run_non_causal_dict[sample] = {}\n",
    "        original_instance = self.x_test.iloc[sample].copy()\n",
    "        contrafactual_set, solution = self.explainerCSSE.explain(original_instance, self.p[sample]) #Method returns the list of counterfactuals and the explanations generated from them\n",
    "\n",
    "        self.run_non_causal_dict[sample]['solution'] = solution\n",
    "\n",
    "    def run_causal(self):\n",
    "        start_time = time.time()\n",
    "        self.run_dict = {}\n",
    "        self.run_dict['global_numbers'] = {\n",
    "                    \"global_quant_changes\": 0,\n",
    "                    \"global_quant_causal_changes\": 0,\n",
    "                    \"global_quant_causal_rules\": 0,\n",
    "                    \"global_quant_zeros_causal\": 0,\n",
    "                    \"global_quant_full_causal\": 0,\n",
    "                    \"global_quant_causal_contrafac\": 0,\n",
    "                    \"global_quant_maioria_causal_satisfeita\": 0,\n",
    "                    \"global_quant_contrafac_unico\": 0,\n",
    "            }\n",
    "        self.global_quant_contrafac_max = 0\n",
    "        if isinstance(self.samples, list):\n",
    "            for sample in self.samples:\n",
    "                self.run_causal_sample(sample)\n",
    "                \n",
    "        elif isinstance(self.samples, int):\n",
    "            for sample in range(self.samples):\n",
    "                try:\n",
    "                    self.run_causal_sample(sample)\n",
    "                except Exception as e:\n",
    "                    print(f\"DEBUG ERRO: {e}\")\n",
    "        \n",
    "        else: \n",
    "            for sample in range(10):\n",
    "                self.run_causal_sample(sample)\n",
    "        \n",
    "        self.global_quant_contrafac_max = self.K * len(self.run_dict)\n",
    "        self.run_dict['global_numbers']['global_timing_run_causal'] = time.time() - start_time\n",
    "\n",
    "\n",
    "    def run_causal_sample(self, sample):\n",
    "        if isinstance(self.samples, list):\n",
    "            original_instance = self.dfx_full.iloc[sample]\n",
    "        else:\n",
    "            original_instance = self.x_test.iloc[sample]\n",
    "        self.run_dict[sample] = {}\n",
    "        self.run_dict[sample]['original_instance'] = original_instance\n",
    "\n",
    "#         print(f'Running original instance:\\n {display(original_instance)}')\n",
    "        print(f'Start to Running samples')\n",
    "\n",
    "        causal_explain = self.get_causal_explain(sample)\n",
    "        self.run_dict[sample]['causal_explain'] = causal_explain\n",
    "\n",
    "        list_analyse = []\n",
    "        for contrafactual in causal_explain[0]:\n",
    "            list_analyse.append(self.analyse_contrafac(contrafactual, causal_explain[1], causal_explain[2]))\n",
    "\n",
    "        self.run_dict[sample]['list_analyse'] = list_analyse\n",
    "        self.analyse_explaination(sample)\n",
    "\n",
    "    def analyse_contrafac(self, contrafac, df, original_ind):\n",
    "        columns = [x.column for x in contrafac]\n",
    "        condicao = (df['to'].isin(columns)) & (df['from'].isin(columns))\n",
    "        ind = original_ind[columns]\n",
    "        return [contrafac, df[condicao], ind]\n",
    "\n",
    "    def get_causal_explain(self, sample):\n",
    "        if isinstance(self.samples, list):\n",
    "            original_ind = self.dfx_full.iloc[sample].copy()\n",
    "        else:\n",
    "            original_ind = self.x_test.iloc[sample].copy() #Original instance\n",
    "        #self.ind_cur_class = ind_cur_class #Index in the shap corresponds to the original instance class\n",
    "        self.explainerCSSE.current_class = self.p[sample] #Original instance class\n",
    "        self.explainerCSSE.original_ind = original_ind\n",
    "        \n",
    "        ind_cur_class = self.explainerCSSE.getBadClass()\n",
    "\n",
    "        #Gets the valid values range of each feature\n",
    "        features_range = []\n",
    "        features_range = self.explainerCSSE.getFeaturesRange()\n",
    "\n",
    "        #The DataFrame df will have the current population\n",
    "        df = pd.DataFrame(columns=self.explainerCSSE.input_dataset.columns)\n",
    "\n",
    "        #Generates the initial population with popinitial mutants        \n",
    "        self.explainerCSSE.getPopInicial(df, features_range)\n",
    "        df_causal = df.copy()\n",
    "        dict_dfs = {}\n",
    "\n",
    "        # for g in tqdm(range(self.explainerCSSE.num_gen), desc= \"Processing...\"):\n",
    "        for g in range(self.generation):\n",
    "\n",
    "            #To use on the parents of each generation\n",
    "            old_parents = pd.DataFrame(columns=self.explainerCSSE.input_dataset.columns)\n",
    "\n",
    "            #Copy parents to the next generation\n",
    "            old_parents = df_causal.copy()\n",
    "            dict_dfs[g] = {}\n",
    "\n",
    "            parents_causal = self.apply_causality(old_parents)\n",
    "            dict_dfs[g]['causal_parents'] = parents_causal\n",
    "            #df will contain the new population\n",
    "            df_causal = pd.DataFrame(columns=self.explainerCSSE.input_dataset.columns)\n",
    "            evaluation_causal = []\n",
    "\n",
    "            #Assessing generation counterfactuals\n",
    "            self.explainerCSSE.fitness(dict_dfs[g]['causal_parents'], evaluation_causal, ind_cur_class)\n",
    "\n",
    "            #The original individual will always be in the 0 position of the df - So that it is normalized too (it will be used later in the distance function)\n",
    "            df_causal.loc[0] = original_ind.copy()\n",
    "\n",
    "            #Copies to the next generation the per_elit best individuals\n",
    "            self.explainerCSSE.elitism(evaluation_causal, df_causal, parents_causal)\n",
    "            number_cross_repetitions = 0\n",
    "            while len(df_causal) < self.explainerCSSE.pop_size + 1: #+1, as the 1st position is used to store the reference individual\n",
    "                number_cross_repetitions_causal = self.explainerCSSE.crossover(df_causal, parents_causal, evaluation_causal, number_cross_repetitions)\n",
    "\n",
    "                mutation_op = rnd.random()\n",
    "                if mutation_op <= self.explainerCSSE.mutation_proba:\n",
    "                    self.explainerCSSE.mutation(df_causal, len(df_causal) - 1, features_range)\n",
    "\n",
    "\n",
    "        evaluation = []\n",
    "        evaluation_causal = []\n",
    "\n",
    "        #Evaluating the latest generation\n",
    "        self.explainerCSSE.fitness(df_causal, evaluation_causal, ind_cur_class)\n",
    "\n",
    "        #Order the last generation by distance to the original instance     \n",
    "        evaluation_causal.sort(key=lambda individual: individual.aval_norm) \n",
    "\n",
    "        #Getting the counterfactual CAUSAL set\n",
    "        contrafactual_set_causal, solution_list_causal = self.explainerCSSE.getContrafactual(df_causal, evaluation_causal) \n",
    "\n",
    "        dict_dfs['contrafactual_set_causal'] = contrafactual_set_causal\n",
    "        dict_dfs['solution_list_causal'] = solution_list_causal\n",
    "        \n",
    "        df_contrafac_causal = self.get_contrafac_df_causal(solution_list_causal)\n",
    "        return [solution_list_causal, df_contrafac_causal, original_ind]\n",
    "    \n",
    "\n",
    "    def apply_causality(self, df):\n",
    "        df_apply_causal = pd.DataFrame(columns = df.columns)\n",
    "        original = df.iloc[0]\n",
    "        df_apply_causal.loc[0] = original\n",
    "        for index, df_row in df.iloc[1:].iterrows():\n",
    "            causal_ind = df_row.copy()\n",
    "            for column in self.causal_order:\n",
    "                value_diff = causal_ind[column] - original[column]\n",
    "                if value_diff != 0:\n",
    "                    tmp_effects = self.df_causal_effects[self.df_causal_effects['from'] == column]\n",
    "                    for index, row in tmp_effects.iterrows():\n",
    "    #                     prob = rnd.random()\n",
    "    #                     if row['probability'] <= prob:\n",
    "                        tmp_error = self.df_error[self.df_error['from'].isin([column, row['to']]) | self.df_error['to'].isin([column, row['to']])]\n",
    "                        error_value = tmp_error['effect'].iloc[0]\n",
    "    #                     print(f'error value = {error_value}')\n",
    "                        causal_ind[row['to']] = causal_ind[row['to']] + (value_diff * row['effect']) + tmp_error['effect'].iloc[0]\n",
    "            df_apply_causal.loc[len(df_apply_causal)] = causal_ind\n",
    "        return df_apply_causal\n",
    "\n",
    "\n",
    "    def get_contrafac_df_causal(self, solution_list_causal):\n",
    "        lista_solution_causal = [[t.column for t in sublist] for sublist in solution_list_causal]\n",
    "\n",
    "        # Inicializa uma lista para armazenar os resultados\n",
    "        resultados = []\n",
    "\n",
    "        # Loop sobre os valores na lista\n",
    "        for lista_valores in lista_solution_causal:\n",
    "            if len(lista_valores) > 1:\n",
    "                for v1 in lista_valores:\n",
    "                    for v2 in lista_valores:\n",
    "                        if v1 != v2:\n",
    "                            # Cria uma condição para cada par de valores diferentes na lista\n",
    "                            condicao = (self.df_causal_effects['to'].isin([v1, v2])) & (self.df_causal_effects['from'].isin([v1, v2]))\n",
    "                            # Realiza a busca no DataFrame usando a condição e armazena os resultados\n",
    "                            resultados.append(self.df_causal_effects[condicao])\n",
    "\n",
    "        # Concatena os resultados em um único DataFrame\n",
    "        if resultados:\n",
    "            resultado_final = pd.concat(resultados)\n",
    "            resultado_final = resultado_final.drop_duplicates()\n",
    "        else:\n",
    "            resultado_final = pd.DataFrame(columns = self.df_causal_effects.columns)\n",
    "            \n",
    "        return resultado_final\n",
    "    \n",
    "\n",
    "    def analyse_explaination(self, sample):\n",
    "        self.run_dict[sample]['data_analysis'] = []\n",
    "        for i, content in enumerate(self.run_dict[sample]['list_analyse']):\n",
    "            self.global_quant_contrafac_max += 1\n",
    "            controle = {}\n",
    "            causal = content[0]\n",
    "            df = content[1]\n",
    "            ori = content[2]\n",
    "            \n",
    "            \n",
    "            num_changes = len(causal)\n",
    "            self.run_dict['global_numbers']['global_quant_changes'] += num_changes\n",
    "            \n",
    "            num_causal_rules = len(df)\n",
    "            self.run_dict['global_numbers']['global_quant_causal_rules'] += num_causal_rules\n",
    "            \n",
    "            for attr in causal:\n",
    "                key = attr.column\n",
    "                if attr.value > ori[key]:\n",
    "                    controle[key] = 'mais'\n",
    "                else:\n",
    "                    controle[key] = 'menos'\n",
    "\n",
    "            df_temp = df.copy()\n",
    "            df_temp['from'] = df['from'].map(controle)\n",
    "            df_temp['to'] = df['to'].map(controle)\n",
    "            if len(df_temp) > 0:\n",
    "                df_temp['causal'] = df_temp.apply(lambda row: self.verificar_condicoes(row), axis = 1)\n",
    "                causal_finds = df_temp['causal'].sum()\n",
    "            else:\n",
    "                causal_finds = 0\n",
    "                \n",
    "            data_dict = {}\n",
    "\n",
    "            data_dict['df_respeita_causal'] = df_temp\n",
    "            data_dict['contrafactual_causal'] = causal\n",
    "            data_dict['df_causal_effects'] = df\n",
    "            \n",
    "            self.run_dict[sample]['data_analysis'].append(data_dict)\n",
    "\n",
    "            self.run_dict['global_numbers']['global_quant_causal_changes'] += causal_finds\n",
    "            \n",
    "            # print(f'causal = \\n{causal}\\n')\n",
    "            # print(f'original = \\n{ori}\\n')\n",
    "            # print(f'df_temp = \\n{display(df_temp)}\\n')\n",
    "            \n",
    "            if len(df_temp) > 0:\n",
    "                if causal_finds > 0:\n",
    "                    self.run_dict['global_numbers']['global_quant_causal_contrafac'] += 1\n",
    "                else:\n",
    "                    # print(f'nenhuma relaçao causal satisfeita')\n",
    "                    self.run_dict['global_numbers']['global_quant_zeros_causal'] += 1\n",
    "    #                 display(df_temp)\n",
    "    #                 print(f\"original = {ori}\")\n",
    "    #                 print(f\"causal = {causal}\")\n",
    "\n",
    "                if causal_finds == num_causal_rules:\n",
    "                    self.run_dict['global_numbers']['global_quant_full_causal'] += 1\n",
    "                    # if causal_finds > 2:\n",
    "                        # print(f'todas > 2 relaçoes causais satisfeitas')\n",
    "    #                     display(df_temp)\n",
    "    #                     print(f\"original = {ori}\")\n",
    "    #                     print(f\"causal = {causal}\")\n",
    "                    # elif causal_finds == 1:\n",
    "                        # print(f'todas = 1 relaçoes causais satisfeitas')\n",
    "                \n",
    "                if causal_finds >= (len(df_temp)/2):\n",
    "                    self.run_dict['global_numbers']['global_quant_maioria_causal_satisfeita'] += 1\n",
    "            else:\n",
    "    #             if len(causal) > 0:\n",
    "                self.run_dict['global_numbers']['global_quant_contrafac_unico'] += 1\n",
    "        \n",
    "    def verificar_condicoes(self, row):\n",
    "        if (row['from'] == 'mais' and row['to'] == 'mais' and row['effect'] > 0):\n",
    "            return True\n",
    "        elif row['from'] == 'menos' and row['to'] == 'menos' and row['effect'] > 0:\n",
    "            return True\n",
    "        elif row['from'] == 'mais' and row['to'] == 'menos' and row['effect'] < 0:\n",
    "            return True\n",
    "        elif row['from'] == 'menos' and row['to'] == 'mais' and row['effect'] < 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "            \n",
    "\n",
    "    def show_metrics(self, get_output = False):\n",
    "        \n",
    "        print(f\"Quantidade de instâncias contrafactuais = {self.global_quant_contrafac_max}\")\n",
    "        print(f'Quantidade de relações causais na base de dados = {len(self.df_causal_effects)}')\n",
    "        print(f\"Quantidade de atributos modificados = {self.run_dict['global_numbers']['global_quant_changes']}\")\n",
    "        print(f\"Quantidade de instâncias contrafactuais causais = {self.run_dict['global_numbers']['global_quant_contrafac_unico'] + self.run_dict['global_numbers']['global_quant_causal_contrafac']}\")\n",
    "        print(f\"Quantidade de relações causais analisadas = {self.run_dict['global_numbers']['global_quant_causal_rules']}\")\n",
    "        print(f\"Quantidade de relações causais satisfeitas = {self.run_dict['global_numbers']['global_quant_causal_changes']}\")\n",
    "        print(f\"Quantidade de instâncias contrafactuais com um único atributo modificado = {self.run_dict['global_numbers']['global_quant_contrafac_unico']}\")\n",
    "        print(f\"Tempo de execução = {self.run_dict['global_numbers']['global_timing_run_causal']}\")\n",
    "        \n",
    "        if get_output:\n",
    "            metrics_dict = {\n",
    "                \"Quantidade de instâncias contrafactuais\": self.global_quant_contrafac_max,\n",
    "                \"Quantidade de relações causais na base de dados\": len(self.df_causal_effects),\n",
    "                \"Quantidade de atributos modificados\": self.run_dict['global_numbers']['global_quant_changes'],\n",
    "                \"Quantidade de instâncias contrafactuais causais\": self.run_dict['global_numbers']['global_quant_contrafac_unico'] + self.run_dict['global_numbers']['global_quant_causal_contrafac'],\n",
    "                \"Quantidade de relações causais analisadas\": self.run_dict['global_numbers']['global_quant_causal_rules'],\n",
    "                \"Quantidade de relações causais satisfeitas\": self.run_dict['global_numbers']['global_quant_causal_changes'],\n",
    "                \"Quantidade de instâncias contrafactuais com um único atributo modificado\": self.run_dict['global_numbers']['global_quant_contrafac_unico'],\n",
    "                \"Tempo de execução\": self.run_dict['global_numbers']['global_timing_run_causal']\n",
    "            }\n",
    "        \n",
    "            return metrics_dict\n",
    "\n",
    "def get_causal_metrics(row, bb_model_name):\n",
    "#     try:\n",
    "    print(row)\n",
    "    ccsse = CCSSE(row['name'], samples = 10, K = 10, generation= 10, bb_model = bb_model_name)\n",
    "    print(f\"criou o modelo\")\n",
    "    ccsse.run_causal()\n",
    "    print(f\"run_causal completo\")\n",
    "    dict_metricas = ccsse.show_metrics(get_output = True)\n",
    "    converted_dict_metricas = convert_np_types(dict_metricas)\n",
    "    json_data = json.dumps(converted_dict_metricas, indent=4)\n",
    "\n",
    "    s3.put_object(Bucket='omar-testes-gerais', Key=f'artigos/causal_csse/bateria_metricas/outputs/metricas_1/{bb_model_name}/{row[\"name\"]}.json', Body=json_data)\n",
    "    print(f\"Execução completa para {row['name']}\")\n",
    "#     except Exception as e:\n",
    "#         print(f'Execução falhou - Nome da base de dados: {row[\"name\"]}')\n",
    "#         print(e)\n",
    "\n",
    "def convert_np_types(data):\n",
    "    \"\"\"Converte tipos de dados NumPy em tipos nativos do Python.\"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        return {key: convert_np_types(value) for key, value in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [convert_np_types(item) for item in data]\n",
    "    elif isinstance(data, np.int64):\n",
    "        return int(data)  # Converte int64 para int\n",
    "    elif isinstance(data, np.float64):\n",
    "        return float(data)  # Converte float64 para float\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "#PJ LOCAL\n",
    "def handler(dataset, model_name):\n",
    "    \n",
    "    args = {\n",
    "        \"list_dataset_name\": dataset,\n",
    "        'bb_model_name': model_name\n",
    "    }\n",
    "    \n",
    "    df_map_inference_datasets = pd.read_parquet(f\"s3://omar-testes-gerais/artigos/artifacts/df_map_inference_datasets.parquet\")\n",
    "    df_dataset = df_map_inference_datasets[df_map_inference_datasets['name'].isin(args[\"list_dataset_name\"])]\n",
    "    df_dataset.apply(lambda x: get_causal_metrics(x, args[\"bb_model_name\"]), axis = 1)\n",
    "\n",
    "#PJ REAL\n",
    "# if __name__ == '__main__':\n",
    "    \n",
    "#     parser = argparse.ArgumentParser()\n",
    "    \n",
    "#     # Adicionando os argumentos para input e output\n",
    "#     parser.add_argument('--list_dataset_name', type=str)\n",
    "#     parser.add_argument('--bb_model_name', type=str)\n",
    "    \n",
    "#     args = parser.parse_args()\n",
    "#     list_dataset_name = ast.literal_eval(args.list_dataset_name)\n",
    "\n",
    "#     df_map_inference_datasets = pd.read_parquet(f\"s3://omar-testes-gerais/artigos/artifacts/df_map_inference_datasets.parquet\")\n",
    "#     df_dataset = df_map_inference_datasets[df_map_inference_datasets['name'].isin(list_dataset_name)]\n",
    "#     df_dataset.apply(lambda x: get_causal_metrics(x, args.bb_model_name), axis = 1)\n",
    "    return df_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3843574b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name                 Musk\n",
      "path      Musk/clean1.csv\n",
      "classe          Column167\n",
      "Name: 32, dtype: object\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of continuous and binary targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mhandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMusk\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 1207\u001b[0m, in \u001b[0;36mhandler\u001b[0;34m(dataset, model_name)\u001b[0m\n\u001b[1;32m   1205\u001b[0m     df_map_inference_datasets \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3://omar-testes-gerais/artigos/artifacts/df_map_inference_datasets.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1206\u001b[0m     df_dataset \u001b[38;5;241m=\u001b[39m df_map_inference_datasets[df_map_inference_datasets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlist_dataset_name\u001b[39m\u001b[38;5;124m\"\u001b[39m])]\n\u001b[0;32m-> 1207\u001b[0m     \u001b[43mdf_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_causal_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbb_model_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1209\u001b[0m \u001b[38;5;66;03m#PJ REAL\u001b[39;00m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;66;03m# if __name__ == '__main__':\u001b[39;00m\n\u001b[1;32m   1211\u001b[0m     \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1222\u001b[0m \u001b[38;5;66;03m#     df_dataset = df_map_inference_datasets[df_map_inference_datasets['name'].isin(list_dataset_name)]\u001b[39;00m\n\u001b[1;32m   1223\u001b[0m \u001b[38;5;66;03m#     df_dataset.apply(lambda x: get_causal_metrics(x, args.bb_model_name), axis = 1)\u001b[39;00m\n\u001b[1;32m   1224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df_dataset\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/frame.py:10374\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m  10360\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m  10362\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m  10363\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  10364\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10372\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m  10373\u001b[0m )\n\u001b[0;32m> 10374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[0;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[12], line 1207\u001b[0m, in \u001b[0;36mhandler.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1205\u001b[0m     df_map_inference_datasets \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3://omar-testes-gerais/artigos/artifacts/df_map_inference_datasets.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1206\u001b[0m     df_dataset \u001b[38;5;241m=\u001b[39m df_map_inference_datasets[df_map_inference_datasets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlist_dataset_name\u001b[39m\u001b[38;5;124m\"\u001b[39m])]\n\u001b[0;32m-> 1207\u001b[0m     df_dataset\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mget_causal_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbb_model_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   1209\u001b[0m \u001b[38;5;66;03m#PJ REAL\u001b[39;00m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;66;03m# if __name__ == '__main__':\u001b[39;00m\n\u001b[1;32m   1211\u001b[0m     \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1222\u001b[0m \u001b[38;5;66;03m#     df_dataset = df_map_inference_datasets[df_map_inference_datasets['name'].isin(list_dataset_name)]\u001b[39;00m\n\u001b[1;32m   1223\u001b[0m \u001b[38;5;66;03m#     df_dataset.apply(lambda x: get_causal_metrics(x, args.bb_model_name), axis = 1)\u001b[39;00m\n\u001b[1;32m   1224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df_dataset\n",
      "Cell \u001b[0;32mIn[12], line 1170\u001b[0m, in \u001b[0;36mget_causal_metrics\u001b[0;34m(row, bb_model_name)\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_causal_metrics\u001b[39m(row, bb_model_name):\n\u001b[1;32m   1168\u001b[0m \u001b[38;5;66;03m#     try:\u001b[39;00m\n\u001b[1;32m   1169\u001b[0m     \u001b[38;5;28mprint\u001b[39m(row)\n\u001b[0;32m-> 1170\u001b[0m     ccsse \u001b[38;5;241m=\u001b[39m \u001b[43mCCSSE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbb_model\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbb_model_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1171\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcriou o modelo\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1172\u001b[0m     ccsse\u001b[38;5;241m.\u001b[39mrun_causal()\n",
      "Cell \u001b[0;32mIn[12], line 523\u001b[0m, in \u001b[0;36mCCSSE.__init__\u001b[0;34m(self, dataset, bb_model, samples, K, generation)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;66;03m#         self.x_train, self.x_test, self.y_train, self.y_test, self.dfx_full, self.dfy_full = self.get_datasets_train_test()\u001b[39;00m\n\u001b[1;32m    521\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_train, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_test, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_train, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_test, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdfx_full, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdfy_full \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_dataset()\n\u001b[0;32m--> 523\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbb_model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_bb_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbb_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    524\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexplainerCSSE \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_model_contrafactual()\n\u001b[1;32m    525\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_causal, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf_causal_effects, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf_error, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcausal_order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_model_causality()\n",
      "Cell \u001b[0;32mIn[12], line 788\u001b[0m, in \u001b[0;36mCCSSE.get_bb_model\u001b[0;34m(self, bb_model_name)\u001b[0m\n\u001b[1;32m    784\u001b[0m     bb_model\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_train, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_train)\n\u001b[1;32m    786\u001b[0m     p \u001b[38;5;241m=\u001b[39m bb_model\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_test)\n\u001b[0;32m--> 788\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mclassification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bb_model, p\n\u001b[1;32m    791\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m bb_model_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrn\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:2612\u001b[0m, in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m   2478\u001b[0m     {\n\u001b[1;32m   2479\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2503\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2504\u001b[0m ):\n\u001b[1;32m   2505\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a text report showing the main classification metrics.\u001b[39;00m\n\u001b[1;32m   2506\u001b[0m \n\u001b[1;32m   2507\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <classification_report>`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2609\u001b[0m \u001b[38;5;124;03m    <BLANKLINE>\u001b[39;00m\n\u001b[1;32m   2610\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2612\u001b[0m     y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2614\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2615\u001b[0m         labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:108\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m    105\u001b[0m     y_type \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_type) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    109\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification metrics can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt handle a mix of \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m targets\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    110\u001b[0m             type_true, type_pred\n\u001b[1;32m    111\u001b[0m         )\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[1;32m    115\u001b[0m y_type \u001b[38;5;241m=\u001b[39m y_type\u001b[38;5;241m.\u001b[39mpop()\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of continuous and binary targets"
     ]
    }
   ],
   "source": [
    "df_dataset = handler([\"Musk\"], 'rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35938695",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
