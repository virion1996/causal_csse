{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a2af60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Using cached numpy-2.2.6-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\omarkrauss\\desktop\\pessoal\\tcc\\tcc\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\omarkrauss\\desktop\\pessoal\\tcc\\tcc\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "Using cached numpy-2.2.6-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-2.2.6 pandas-2.2.3 pytz-2025.2 tzdata-2025.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.6.1-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\omarkrauss\\desktop\\pessoal\\tcc\\tcc\\.venv\\lib\\site-packages (from scikit-learn) (2.2.6)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Using cached scipy-1.15.3-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.5.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached scikit_learn-1.6.1-cp312-cp312-win_amd64.whl (11.1 MB)\n",
      "Using cached joblib-1.5.0-py3-none-any.whl (307 kB)\n",
      "Using cached scipy-1.15.3-cp312-cp312-win_amd64.whl (41.0 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.5.0 scikit-learn-1.6.1 scipy-1.15.3 threadpoolctl-3.6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lingam\n",
      "  Downloading lingam-1.9.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\omarkrauss\\desktop\\pessoal\\tcc\\tcc\\.venv\\lib\\site-packages (from lingam) (2.2.6)\n",
      "Requirement already satisfied: scipy in c:\\users\\omarkrauss\\desktop\\pessoal\\tcc\\tcc\\.venv\\lib\\site-packages (from lingam) (1.15.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\omarkrauss\\desktop\\pessoal\\tcc\\tcc\\.venv\\lib\\site-packages (from lingam) (1.6.1)\n",
      "Collecting graphviz (from lingam)\n",
      "  Downloading graphviz-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting statsmodels (from lingam)\n",
      "  Downloading statsmodels-0.14.4-cp312-cp312-win_amd64.whl.metadata (9.5 kB)\n",
      "Collecting networkx (from lingam)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\omarkrauss\\desktop\\pessoal\\tcc\\tcc\\.venv\\lib\\site-packages (from lingam) (2.2.3)\n",
      "Collecting pygam (from lingam)\n",
      "  Downloading pygam-0.9.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting matplotlib (from lingam)\n",
      "  Using cached matplotlib-3.10.3-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Collecting psy (from lingam)\n",
      "  Downloading psy-0.0.1-py2.py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting semopy (from lingam)\n",
      "  Downloading semopy-2.3.11.tar.gz (1.6 MB)\n",
      "     ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "     ---------------------------------------- 1.6/1.6 MB 8.0 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->lingam)\n",
      "  Using cached contourpy-1.3.2-cp312-cp312-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->lingam)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->lingam)\n",
      "  Using cached fonttools-4.58.0-cp312-cp312-win_amd64.whl.metadata (106 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->lingam)\n",
      "  Using cached kiwisolver-1.4.8-cp312-cp312-win_amd64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\omarkrauss\\desktop\\pessoal\\tcc\\tcc\\.venv\\lib\\site-packages (from matplotlib->lingam) (25.0)\n",
      "Collecting pillow>=8 (from matplotlib->lingam)\n",
      "  Using cached pillow-11.2.1-cp312-cp312-win_amd64.whl.metadata (9.1 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib->lingam)\n",
      "  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\omarkrauss\\desktop\\pessoal\\tcc\\tcc\\.venv\\lib\\site-packages (from matplotlib->lingam) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\omarkrauss\\desktop\\pessoal\\tcc\\tcc\\.venv\\lib\\site-packages (from pandas->lingam) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\omarkrauss\\desktop\\pessoal\\tcc\\tcc\\.venv\\lib\\site-packages (from pandas->lingam) (2025.2)\n",
      "Collecting progressbar2 (from psy->lingam)\n",
      "  Downloading progressbar2-4.5.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting scipy (from lingam)\n",
      "  Downloading scipy-1.11.4-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting numpy (from lingam)\n",
      "  Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\omarkrauss\\desktop\\pessoal\\tcc\\tcc\\.venv\\lib\\site-packages (from scikit-learn->lingam) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\omarkrauss\\desktop\\pessoal\\tcc\\tcc\\.venv\\lib\\site-packages (from scikit-learn->lingam) (3.6.0)\n",
      "Collecting sympy (from semopy->lingam)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting numdifftools (from semopy->lingam)\n",
      "  Downloading numdifftools-0.9.41-py2.py3-none-any.whl.metadata (39 kB)\n",
      "Collecting patsy>=0.5.6 (from statsmodels->lingam)\n",
      "  Downloading patsy-1.0.1-py2.py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting python-utils>=3.8.1 (from progressbar2->psy->lingam)\n",
      "  Downloading python_utils-3.9.1-py2.py3-none-any.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\omarkrauss\\desktop\\pessoal\\tcc\\tcc\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->lingam) (1.17.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->semopy->lingam)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting typing_extensions>3.10.0.2 (from python-utils>=3.8.1->progressbar2->psy->lingam)\n",
      "  Using cached typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Downloading lingam-1.9.1-py3-none-any.whl (103 kB)\n",
      "Downloading graphviz-0.20.3-py3-none-any.whl (47 kB)\n",
      "Using cached matplotlib-3.10.3-cp312-cp312-win_amd64.whl (8.1 MB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Downloading psy-0.0.1-py2.py3-none-any.whl (38 kB)\n",
      "Downloading pygam-0.9.1-py3-none-any.whl (522 kB)\n",
      "Downloading scipy-1.11.4-cp312-cp312-win_amd64.whl (43.7 MB)\n",
      "   ---------------------------------------- 0.0/43.7 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 5.2/43.7 MB 45.7 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 12.6/43.7 MB 31.6 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 22.0/43.7 MB 35.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 32.5/43.7 MB 39.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 37.7/43.7 MB 36.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 43.7/43.7 MB 36.6 MB/s eta 0:00:00\n",
      "Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl (15.5 MB)\n",
      "Downloading statsmodels-0.14.4-cp312-cp312-win_amd64.whl (9.8 MB)\n",
      "   ---------------------------------------- 0.0/9.8 MB ? eta -:--:--\n",
      "   ---------------------------------------  9.7/9.8 MB 54.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.8/9.8 MB 47.1 MB/s eta 0:00:00\n",
      "Using cached contourpy-1.3.2-cp312-cp312-win_amd64.whl (223 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.58.0-cp312-cp312-win_amd64.whl (2.2 MB)\n",
      "Using cached kiwisolver-1.4.8-cp312-cp312-win_amd64.whl (71 kB)\n",
      "Downloading patsy-1.0.1-py2.py3-none-any.whl (232 kB)\n",
      "Using cached pillow-11.2.1-cp312-cp312-win_amd64.whl (2.7 MB)\n",
      "Downloading progressbar2-4.5.0-py3-none-any.whl (57 kB)\n",
      "Using cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Downloading numdifftools-0.9.41-py2.py3-none-any.whl (100 kB)\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 6.3/6.3 MB 42.8 MB/s eta 0:00:00\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Downloading python_utils-3.9.1-py2.py3-none-any.whl (32 kB)\n",
      "Using cached typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "Building wheels for collected packages: semopy\n",
      "  Building wheel for semopy (pyproject.toml): started\n",
      "  Building wheel for semopy (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for semopy: filename=semopy-2.3.11-py3-none-any.whl size=1659795 sha256=a074d54fc30d0812840f2476c8497abb0982067d7aa7e0b5ac5493871fdc64fa\n",
      "  Stored in directory: c:\\users\\omarkrauss\\appdata\\local\\pip\\cache\\wheels\\c6\\24\\8b\\be911b059a61f490f38425eb19bf2fed470a5ead97228e8255\n",
      "Successfully built semopy\n",
      "Installing collected packages: mpmath, typing_extensions, sympy, pyparsing, pillow, numpy, networkx, kiwisolver, graphviz, fonttools, cycler, scipy, python-utils, patsy, contourpy, statsmodels, progressbar2, numdifftools, matplotlib, semopy, pygam, psy, lingam\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.6\n",
      "    Uninstalling numpy-2.2.6:\n",
      "      Successfully uninstalled numpy-2.2.6\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.15.3\n",
      "    Uninstalling scipy-1.15.3:\n",
      "      Successfully uninstalled scipy-1.15.3\n",
      "Successfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.58.0 graphviz-0.20.3 kiwisolver-1.4.8 lingam-1.9.1 matplotlib-3.10.3 mpmath-1.3.0 networkx-3.4.2 numdifftools-0.9.41 numpy-1.26.4 patsy-1.0.1 pillow-11.2.1 progressbar2-4.5.0 psy-0.0.1 pygam-0.9.1 pyparsing-3.2.3 python-utils-3.9.1 scipy-1.11.4 semopy-2.3.11 statsmodels-0.14.4 sympy-1.14.0 typing_extensions-4.13.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fsspec\n",
      "  Downloading fsspec-2025.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Downloading fsspec-2025.5.0-py3-none-any.whl (196 kB)\n",
      "Installing collected packages: fsspec\n",
      "Successfully installed fsspec-2025.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting s3fs\n",
      "  Downloading s3fs-2025.5.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting aiobotocore<3.0.0,>=2.5.4 (from s3fs)\n",
      "  Downloading aiobotocore-2.22.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: fsspec==2025.5.0 in c:\\users\\omarkrauss\\desktop\\pessoal\\tcc\\tcc\\.venv\\lib\\site-packages (from s3fs) (2025.5.0)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from s3fs)\n",
      "  Downloading aiohttp-3.11.18-cp312-cp312-win_amd64.whl.metadata (8.0 kB)\n",
      "Collecting aioitertools<1.0.0,>=0.5.1 (from aiobotocore<3.0.0,>=2.5.4->s3fs)\n",
      "  Downloading aioitertools-0.12.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting botocore<1.37.4,>=1.37.2 (from aiobotocore<3.0.0,>=2.5.4->s3fs)\n",
      "  Downloading botocore-1.37.3-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\omarkrauss\\desktop\\pessoal\\tcc\\tcc\\.venv\\lib\\site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (2.9.0.post0)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from aiobotocore<3.0.0,>=2.5.4->s3fs)\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting multidict<7.0.0,>=6.0.0 (from aiobotocore<3.0.0,>=2.5.4->s3fs)\n",
      "  Downloading multidict-6.4.4-cp312-cp312-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting wrapt<2.0.0,>=1.10.10 (from aiobotocore<3.0.0,>=2.5.4->s3fs)\n",
      "  Using cached wrapt-1.17.2-cp312-cp312-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs)\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs)\n",
      "  Downloading frozenlist-1.6.0-cp312-cp312-win_amd64.whl.metadata (16 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs)\n",
      "  Using cached propcache-0.3.1-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs)\n",
      "  Downloading yarl-1.20.0-cp312-cp312-win_amd64.whl.metadata (74 kB)\n",
      "Collecting urllib3!=2.2.0,<3,>=1.25.4 (from botocore<1.37.4,>=1.37.2->aiobotocore<3.0.0,>=2.5.4->s3fs)\n",
      "  Using cached urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\omarkrauss\\desktop\\pessoal\\tcc\\tcc\\.venv\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->aiobotocore<3.0.0,>=2.5.4->s3fs) (1.17.0)\n",
      "Collecting idna>=2.0 (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Downloading s3fs-2025.5.0-py3-none-any.whl (30 kB)\n",
      "Downloading aiobotocore-2.22.0-py3-none-any.whl (78 kB)\n",
      "Downloading aiohttp-3.11.18-cp312-cp312-win_amd64.whl (439 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aioitertools-0.12.0-py3-none-any.whl (24 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading botocore-1.37.3-py3-none-any.whl (13.3 MB)\n",
      "   ---------------------------------------- 0.0/13.3 MB ? eta -:--:--\n",
      "   ----------------------------- ---------- 9.7/13.3 MB 46.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.3/13.3 MB 39.9 MB/s eta 0:00:00\n",
      "Downloading frozenlist-1.6.0-cp312-cp312-win_amd64.whl (120 kB)\n",
      "Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading multidict-6.4.4-cp312-cp312-win_amd64.whl (38 kB)\n",
      "Using cached propcache-0.3.1-cp312-cp312-win_amd64.whl (44 kB)\n",
      "Using cached wrapt-1.17.2-cp312-cp312-win_amd64.whl (38 kB)\n",
      "Downloading yarl-1.20.0-cp312-cp312-win_amd64.whl (92 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Installing collected packages: wrapt, urllib3, propcache, multidict, jmespath, idna, frozenlist, attrs, aioitertools, aiohappyeyeballs, yarl, botocore, aiosignal, aiohttp, aiobotocore, s3fs\n",
      "Successfully installed aiobotocore-2.22.0 aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aioitertools-0.12.0 aiosignal-1.3.2 attrs-25.3.0 botocore-1.37.3 frozenlist-1.6.0 idna-3.10 jmespath-1.0.1 multidict-6.4.4 propcache-0.3.1 s3fs-2025.5.0 urllib3-2.4.0 wrapt-1.17.2 yarl-1.20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Using cached pyarrow-20.0.0-cp312-cp312-win_amd64.whl.metadata (3.4 kB)\n",
      "Using cached pyarrow-20.0.0-cp312-cp312-win_amd64.whl (25.7 MB)\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-20.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install scikit-learn\n",
    "!pip install lingam\n",
    "!pip install fsspec\n",
    "!pip install s3fs\n",
    "!pip install pyarrow\n",
    "\n",
    "!pip install ydata-profiling\n",
    "!pip install --upgrade --force-reinstall setuptools\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bc14207",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# os.system(\"pip install lingam\")\n",
    "# os.system(\"pip install fsspec\")\n",
    "# os.system(\"pip install s3fs\")\n",
    "# os.system(\"pip install pyarrow\")\n",
    "\n",
    "import random as rnd\n",
    "from scipy.spatial import distance\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import warnings\n",
    "import sys\n",
    "import argparse\n",
    "import ast\n",
    "import time\n",
    "import json\n",
    "\n",
    "#German\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import graphviz\n",
    "import lingam\n",
    "from lingam.utils import print_causal_directions, print_dagc, make_dot\n",
    "\n",
    "import random as rnd\n",
    "\n",
    "# from IPython.display import display\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# s3 = boto3.client('s3')\n",
    "\n",
    "#Used for ordering evaluations\n",
    "class individual:\n",
    "    def __init__(self, index, score, distance, num_changes, aval_norm, dist_norm, predict_proba):\n",
    "        self.index = index #Indicates the instance's position in the dataframe\n",
    "        self.score = score #Indicates the score in relation to the proximity of the class boundary\n",
    "        self.distance = distance #Indicates the distance from the original instance\n",
    "        self.num_changes = num_changes #Indicates the number of changes for class change\n",
    "        self.aval_norm = aval_norm #Indicates the final fitness with standardized metrics\n",
    "        self.dist_norm = dist_norm #Indicates the normalized distance (distance and number of changes)\n",
    "        self.predict_proba = predict_proba #Indicates de individual's class\n",
    "    def __repr__(self):\n",
    "        return repr((self.index, self.score, self.distance, self.num_changes, self.aval_norm, self.dist_norm, self.predict_proba))\n",
    "\n",
    "class counter_change:\n",
    "    def __init__(self, column, value):\n",
    "        self.column = column \n",
    "        self.value = value\n",
    "    def __eq__(self, other):\n",
    "        if self.column == other.column and self.value == other.value:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    def __repr__(self):\n",
    "        return repr((self.column, self.value))    \n",
    "\n",
    "#Used to generate a random value in the mutation operation\n",
    "class feature_range:\n",
    "    def __init__(self, column, col_type, min_value, max_value):\n",
    "        self.column = column \n",
    "        self.col_type = col_type\n",
    "        self.min_value = min_value\n",
    "        self.max_value = max_value\n",
    "\n",
    "    #Returns a random value to perform mutation operation\n",
    "    def get_random_value(self):\n",
    "        if self.col_type == 'int64' or self.col_type == 'int' or self.col_type == 'int16' or self.col_type == 'int8' or (self.col_type == 'uint8'):\n",
    "            value = rnd.randint(self.min_value, self.max_value)\n",
    "        else:  \n",
    "            value = round(rnd.uniform(self.min_value, self.max_value), 2)\n",
    "        return value\n",
    "    \n",
    "    #Checks if the attribute has only one value.\n",
    "    def unique_value(self):\n",
    "        if self.min_value != self.max_value:\n",
    "            return False\n",
    "        else:  \n",
    "            return True    \n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr((self.column, self.col_type, self.min_value, self.max_value)) \n",
    "        \n",
    "class CSSE(object):\n",
    "    \n",
    "    def __init__(self, input_dataset, model, static_list = [], K = 3, num_gen = 30, pop_size = 100, per_elit = 0.1, cros_proba = 0.8, mutation_proba = 0.1, L1 = 1, L2 = 1):\n",
    "        #User Options\n",
    "        self.static_list = static_list #List of static features\n",
    "        self.K = K #Number of counterfactuals desired\n",
    "        #Model\n",
    "        self.input_dataset = input_dataset\n",
    "        self.model = model\n",
    "        #GA Parameters\n",
    "        self.num_gen = num_gen\n",
    "        self.pop_size = pop_size\n",
    "        self.per_elit = per_elit\n",
    "        self.cros_proba = cros_proba\n",
    "        self.mutation_proba = mutation_proba\n",
    "        #Objective function parameters\n",
    "        self.L1 = L1 #weight assigned the distance to the original instance\n",
    "        self.L2 = L2 #weight assigned the number of changes needed in the original instance   \n",
    "    \n",
    "    #Get which index in the SHAP corresponding to the current class\n",
    "    def getBadClass(self):   \n",
    "        if self.current_class == self.model.classes_[0]:\n",
    "            ind_cur_class = 0\n",
    "        else:\n",
    "            ind_cur_class = 1\n",
    "        \n",
    "        return ind_cur_class\n",
    "    \n",
    "    #Gets the valid values range for each feature\n",
    "    def getFeaturesRange(self):\n",
    "        features_range = []\n",
    "       \n",
    "        for i in range (0, self.input_dataset.columns.size):\n",
    "            col_name = self.input_dataset.columns[i]\n",
    "            col_type = self.input_dataset[col_name].dtype\n",
    "            min_value = min(self.input_dataset[col_name])\n",
    "            max_value = max(self.input_dataset[col_name])\n",
    "            \n",
    "            feature_range_ind = feature_range(col_name, col_type, min_value, max_value)\n",
    "            features_range.append(feature_range_ind)\n",
    "        \n",
    "        return features_range\n",
    "       \n",
    "    def getMutationValue(self, currentValue, index, ind_feature_range):\n",
    "        new_value = ind_feature_range.get_random_value()\n",
    "        \n",
    "        while currentValue == new_value:\n",
    "            new_value = ind_feature_range.get_random_value()\n",
    "        \n",
    "        return new_value\n",
    "    \n",
    "    def equal(self, individual, population):\n",
    "        aux = 0\n",
    "        for i in range ( 1, len(population)):\n",
    "            c = population.loc[i].copy()\n",
    "            dst = distance.euclidean(individual, c)\n",
    "            if dst == 0:\n",
    "                aux = 1\n",
    "        \n",
    "        return aux\n",
    "\n",
    "    def getPopInicial (self, df, features_range): \n",
    "        #The reference individual will always be in the 0 position of the df - so that it is normalized as well (it will be used later in the distance function)\n",
    "        df.loc[0] = self.original_ind.copy()\n",
    "        \n",
    "        #Counting numbers of repeated individuals\n",
    "        number_repetitions = 0\n",
    "        \n",
    "        #One more position is used because the zero position was reserved for the reference individual\n",
    "        while len(df) < self.pop_size + 1:\n",
    "            #Draw a feature to change\n",
    "            index_a = rnd.randint( 0, self.input_dataset.columns.size - 1 )\n",
    "            while df.columns[index_a] in self.static_list:\n",
    "                index_a = rnd.randint( 0, self.input_dataset.columns.size - 1 )\n",
    "                \n",
    "            if not features_range[index_a].unique_value():\n",
    "                #Mutation\n",
    "                mutant = self.original_ind.copy()\n",
    "\n",
    "                new_value =  self.getMutationValue(mutant.iloc[index_a], index_a, features_range[index_a])\n",
    "                mutant.iloc[index_a] = new_value\n",
    "\n",
    "                ni = self.equal(mutant, df)\n",
    "                if ni == 0:\n",
    "                    df.loc[len(df)] = mutant.copy()\n",
    "                else:\n",
    "                    #Assesses whether the GA is producing too many repeated individuals.\n",
    "                    number_repetitions = number_repetitions + 1\n",
    "                    if number_repetitions == 2*self.pop_size:\n",
    "                        self.pop_size = round(self.pop_size - self.pop_size*0.1)\n",
    "                        self.mutation_proba = self.mutation_proba + 0.1\n",
    "                        #print('Adjusting population size...', self.pop_size)\n",
    "                        number_repetitions = 0\n",
    "    \n",
    "    #Complete the standardized proximity and similarity assessments for each individual\n",
    "    def getNormalEvaluation(self, evaluation, aval_norma):\n",
    "        scaler2 = preprocessing.MinMaxScaler()\n",
    "        aval_norma2 = scaler2.fit_transform(aval_norma)\n",
    "    \n",
    "        i = 0\n",
    "        while i < len(evaluation):\n",
    "            evaluation[i].aval_norm = self.L1*aval_norma2[i,0] + self.L2*aval_norma2[i,1] + aval_norma2[i,2]\n",
    "            evaluation[i].dist_norm = self.L1*aval_norma2[i,0] + self.L2*aval_norma2[i,1]\n",
    "        \n",
    "            i = i + 1\n",
    "    \n",
    "    def numChanges(self, ind_con):\n",
    "        num = 0\n",
    "        for i in range(len(self.original_ind)):\n",
    "            if self.original_ind[i] != ind_con[i]:\n",
    "                num = num + 1\n",
    "        \n",
    "        return num\n",
    "        \n",
    "    def fitness(self, population, evaluation, ind_cur_class):\n",
    "        def getProximityEvaluation (proba):\n",
    "            #Penalizes the individual who is in the negative class\n",
    "            if proba < 0.5:\n",
    "                predict_score = 0\n",
    "            else:\n",
    "                predict_score= proba\n",
    "             \n",
    "            return predict_score\n",
    "               \n",
    "        #Calculates similarity to the original instance\n",
    "        def getEvaluationDist (ind, X_train_minmax):\n",
    "            #Normalizes the data so that the different scales do not bias the distance\n",
    "            a = X_train_minmax[0]\n",
    "            b = X_train_minmax[ind]\n",
    "            dst = distance.euclidean(a, b)\n",
    "  \n",
    "            return dst\n",
    "        \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "        \n",
    "            predict_proba = self.model.predict_proba(population)\n",
    "                    \n",
    "        #Calculating the distance between instances\n",
    "        scaler = preprocessing.MinMaxScaler()\n",
    "        X_train_minmax = scaler.fit_transform(population)\n",
    "    \n",
    "        i = 0\n",
    "        aval_norma = [] \n",
    "        while i < len(population):\n",
    "            proximityEvaluation = getProximityEvaluation(predict_proba[i, ind_cur_class])\n",
    "            evaldist = getEvaluationDist(i, X_train_minmax)\n",
    "            #The original individual is in the 1st position\n",
    "            numChanges = self.numChanges(population.loc[i])\n",
    "        \n",
    "            ind = individual(i, proximityEvaluation, evaldist, numChanges, 0, 0, predict_proba[i, ind_cur_class])\n",
    "            aval_norma.append([evaldist, numChanges, proximityEvaluation])\n",
    "            evaluation.append(ind)\n",
    "            i = i + 1\n",
    "\n",
    "        self.getNormalEvaluation(evaluation, aval_norma)\n",
    "       \n",
    "    #Given a counterfactual solution returns the list of modified columns\n",
    "    def getColumns(self, counter_solution):\n",
    "        colums = []\n",
    "        for j in range (0, len(counter_solution)):\n",
    "            colums.append(counter_solution[j].column)\n",
    "        \n",
    "        return colums      \n",
    "             \n",
    "    #Checks if the new solution is contained in the solutions already found\n",
    "    def contained_solution(self, original_instance, current_list, current_column_list, new_solution, new_column_solution):\n",
    "        contained = False\n",
    "        for i in range (0, len(current_list)):              \n",
    "            if set(current_column_list[i]).issubset(new_column_solution):\n",
    "                for j in range (0, len(current_list[i])):\n",
    "                    pos = new_column_solution.index(current_list[i][j].column)\n",
    "                    distancia_a = abs(original_instance[current_list[i][j].column] - current_list[i][j].value)\n",
    "                    distancia_b = abs(original_instance[current_list[i][j].column] - new_solution[pos].value)\n",
    "                    if distancia_b >= distancia_a:\n",
    "                        contained = True\n",
    "\n",
    "        return contained\n",
    "      \n",
    "    def elitism(self, evaluation, df, parents):\n",
    "         \n",
    "        num_elit = round(self.per_elit*self.pop_size)\n",
    "        \n",
    "        aval = []\n",
    "        aval = evaluation.copy()\n",
    "        aval.sort(key=lambda individual: individual.aval_norm)\n",
    "        \n",
    "        #contrafactual_ind = pd.DataFrame(columns=self.input_dataset.columns)\n",
    "        solution_list = []\n",
    "        solution_colums_list = []\n",
    "        \n",
    "        i = 0\n",
    "        numContraf = 0\n",
    "        while i < len(aval) and numContraf <= num_elit + 1:\n",
    "            #Checks if the example belongs to the counterfactual class\n",
    "            if aval[i].predict_proba < 0.5:\n",
    "                ind_changes = []\n",
    "                ind_colums_change = []\n",
    "         \n",
    "                #Gets counterfactual example change list\n",
    "                ind_changes = self.getChanges(aval[i].index, parents)\n",
    "                #Generates the list of columns modified in the counterfactual to check if there is already a solution with that set of columns\n",
    "                ind_colums_change = self.getColumns(ind_changes)\n",
    "                \n",
    "                if ind_colums_change not in solution_colums_list:\n",
    "                    #Check if one solution is a subset of the other\n",
    "                    if not self.contained_solution(self.original_ind, solution_list, solution_colums_list, ind_changes, ind_colums_change):\n",
    "                        #Include counterfactual in the list of examples of the final solution                    \n",
    "                        df.loc[len(df)] = parents.iloc[aval[i].index].copy()                     \n",
    "                                \n",
    "                        #Add to the list of solutions (changes only)       \n",
    "                        solution_list.append(ind_changes)\n",
    "                        #Used to compare with the next counterfactuals (to ensure diversity)\n",
    "                        solution_colums_list.append(ind_colums_change)\n",
    "                                        \n",
    "                        numContraf = numContraf + 1\n",
    "                      \n",
    "            i = i + 1\n",
    "        return solution_list\n",
    "    \n",
    "    def roulette_wheel(self, evaluation):\n",
    "        summation = 0\n",
    "        #Performs roulette wheel to select parents who will undergo genetic operations\n",
    "        for i in range (1, len(evaluation)): \n",
    "            summation = summation + 1/evaluation[i].aval_norm\n",
    "    \n",
    "        roulette = rnd.uniform( 0, summation )\n",
    "    \n",
    "        roulette_score = 1/evaluation[1].aval_norm\n",
    "        i = 1\n",
    "        while roulette_score < roulette:\n",
    "            i += 1\n",
    "            roulette_score += 1/evaluation[i].aval_norm\n",
    "        \n",
    "        return i\n",
    "            \n",
    "    def crossover (self, df, parents, evaluation, number_cross_repetitions):\n",
    "        child = []\n",
    "            \n",
    "        corte = rnd.randint( 0, self.input_dataset.columns.size - 1 )\n",
    "            \n",
    "        index1 = self.roulette_wheel(evaluation)\n",
    "        index2 = self.roulette_wheel(evaluation)\n",
    "        \n",
    "        ind_a = parents.iloc[index1].copy()\n",
    "        ind_b = parents.iloc[index2].copy()\n",
    "            \n",
    "        crossover_op = rnd.random()\n",
    "        if crossover_op <= self.cros_proba:\n",
    "            child[ :corte ] = ind_a[ :corte ].copy()\n",
    "            child[ corte: ] = ind_b[ corte: ].copy()\n",
    "        else:\n",
    "            child = ind_a.copy()\n",
    "        \n",
    "        ni = self.equal(child, df)\n",
    "        if ni == 0:\n",
    "            df.loc[len(df)] = child.copy()\n",
    "        else:\n",
    "            #Assesses whether the GA is producing too many repeated individuals.\n",
    "            number_cross_repetitions = number_cross_repetitions + 1\n",
    "            if number_cross_repetitions == self.pop_size:\n",
    "                self.pop_size = round(self.pop_size - self.pop_size*0.1)\n",
    "                self.mutation_proba = self.mutation_proba + 0.1\n",
    "                #print('Adjusting population size...', self.pop_size)\n",
    "                number_cross_repetitions = 0\n",
    "        #    print('repeated')\n",
    "        return number_cross_repetitions\n",
    "                       \n",
    "    def mutation (self, df, individual_pos, features_range):\n",
    "        ni = 1\n",
    "        #Does not allow repeated individual\n",
    "        while ni == 1:\n",
    "            #Draw a feature to change\n",
    "            index_a = rnd.randint( 0, self.input_dataset.columns.size - 1 )\n",
    "            while df.columns[index_a] in self.static_list:\n",
    "                index_a = rnd.randint( 0, self.input_dataset.columns.size - 1 )\n",
    "            \n",
    "            if not features_range[index_a].unique_value():\n",
    "                #Mutation\n",
    "                mutant = df.iloc[individual_pos].copy()\n",
    "            \n",
    "                #Draw the value to be changed\n",
    "                new_value =  self.getMutationValue(mutant.iloc[index_a], index_a, features_range[index_a])  \n",
    "                mutant.iloc[index_a] = new_value\n",
    "\n",
    "                ni = self.equal(mutant, df)\n",
    "                if ni == 0:\n",
    "                    df.loc[individual_pos] = mutant.copy()\n",
    "                #else:\n",
    "                #    print('repeated')\n",
    "     \n",
    "    def getChanges(self, ind, dfComp):\n",
    "        changes = []\n",
    "        \n",
    "        for i in range (len(dfComp.iloc[ind])):\n",
    "            if self.original_ind[i] != dfComp.loc[ind][i]:\n",
    "                counter_change_ind = counter_change(dfComp.columns[i], dfComp.loc[ind][i])\n",
    "                changes.append(counter_change_ind)\n",
    "\n",
    "        return changes\n",
    "    \n",
    "    #Generates the solution from the final population\n",
    "    def getContrafactual(self, df, aval):\n",
    "        \n",
    "        contrafactual_ind = pd.DataFrame(columns=self.input_dataset.columns)\n",
    "        solution_list = []\n",
    "        solution_colums_list = []\n",
    "        \n",
    "        i = 0\n",
    "        numContraf = 0\n",
    "        while i < len(aval) and numContraf < self.K:\n",
    "            #Checks if the example belongs to the counterfactual class\n",
    "            if aval[i].predict_proba < 0.5:\n",
    "                ind_changes = []\n",
    "                ind_colums_change = []\n",
    "         \n",
    "                #Gets counterfactual example change list\n",
    "                ind_changes = self.getChanges(aval[i].index, df)\n",
    "                #Generates the list of columns modified in the counterfactual to check if there is already a solution with that set of columns\n",
    "                ind_colums_change = self.getColumns(ind_changes)\n",
    "                \n",
    "                if ind_colums_change not in solution_colums_list:\n",
    "                    #Check if one solution is a subset of the other\n",
    "                    if not self.contained_solution(self.original_ind, solution_list, solution_colums_list, ind_changes, ind_colums_change):\n",
    "                        #Include counterfactual in the list of examples of the final solution\n",
    "                        contrafactual_ind.loc[len(contrafactual_ind)] = df.iloc[aval[i].index].copy()\n",
    "                                \n",
    "                        #Add to the list of solutions (changes only)       \n",
    "                        solution_list.append(ind_changes)\n",
    "                        #Used to compare with the next counterfactuals (to ensure diversity)\n",
    "                        solution_colums_list.append(ind_colums_change)\n",
    "                                        \n",
    "                        numContraf = numContraf + 1\n",
    "                        #print('solution_list ', solution_list)\n",
    "                    #else:\n",
    "                        #print('is contained ', ind_changes)\n",
    "                #else:\n",
    "                    #print('repeated ', ind_changes)\n",
    "                      \n",
    "            i = i + 1\n",
    "\n",
    "        return contrafactual_ind, solution_list   \n",
    "    \n",
    "    def printResults(self, solution):\n",
    "        print(\"Result obtained\")\n",
    "        if len(solution) != 0:\n",
    "            for i in range(0, len(solution)): \n",
    "                print(\"\\n\")\n",
    "                print(f\"{'Counterfactual ' + str(i + 1):^34}\")\n",
    "                for j in range(0, len(solution[i])): \n",
    "                    print(f\"{str(solution[i][j].column):<29} {str(solution[i][j].value):>5}\")\n",
    "        else:\n",
    "            print('Solution not found. It may be necessary to adjust the parameters for this instance.')\n",
    "                                                 \n",
    "    def explain(self, original_ind, current_class):\n",
    "        self.original_ind = original_ind #Original instance\n",
    "        #self.ind_cur_class = ind_cur_class #Index in the shap corresponds to the original instance class\n",
    "        self.current_class = current_class #Original instance class\n",
    "        \n",
    "        ind_cur_class = self.getBadClass()\n",
    "    \n",
    "        #Gets the valid values range of each feature\n",
    "        features_range = []\n",
    "        features_range = self.getFeaturesRange()\n",
    "\n",
    "        #The DataFrame df will have the current population\n",
    "        df = pd.DataFrame(columns=self.input_dataset.columns)\n",
    "        \n",
    "        #Generates the initial population with popinitial mutants        \n",
    "        self.getPopInicial(df, features_range)\n",
    "        for g in range(self.num_gen):\n",
    "            #To use on the parents of each generation\n",
    "            parents = pd.DataFrame(columns=self.input_dataset.columns)\n",
    "    \n",
    "            #Copy parents to the next generation\n",
    "            parents = df.copy()\n",
    "            #df will contain the new population\n",
    "            df = pd.DataFrame(columns=self.input_dataset.columns)\n",
    "            \n",
    "            evaluation = []                         \n",
    "                   \n",
    "            #Assessing generation counterfactuals\n",
    "            self.fitness(parents, evaluation, ind_cur_class)\n",
    "            #The original individual will always be in the 0 position of the df - So that it is normalized too (it will be used later in the distance function)\n",
    "            df.loc[0] = self.original_ind.copy()\n",
    "            \n",
    "            #Copies to the next generation the per_elit best individuals\n",
    "            self.elitism(evaluation, df, parents)\n",
    "            number_cross_repetitions = 0\n",
    "            while len(df) < self.pop_size + 1: #+1, as the 1st position is used to store the reference individual\n",
    "                number_cross_repetitions = self.crossover(df, parents, evaluation, number_cross_repetitions)\n",
    "                \n",
    "                mutation_op = rnd.random()\n",
    "                if mutation_op <= self.mutation_proba:\n",
    "                    self.mutation(df, len(df) - 1, features_range)\n",
    "            \n",
    "            print()\n",
    "                 \n",
    "        evaluation = []\n",
    "    \n",
    "        #Evaluating the latest generation\n",
    "        self.fitness(df, evaluation, ind_cur_class)\n",
    "    \n",
    "        #Order the last generation by distance to the original instance     \n",
    "        evaluation.sort(key=lambda individual: individual.aval_norm)     \n",
    "        \n",
    "        #Getting the counterfactual set\n",
    "        contrafactual_set = pd.DataFrame(columns=self.input_dataset.columns)\n",
    "        contrafactual_set, solution_list = self.getContrafactual(df, evaluation)       \n",
    "                 \n",
    "        return contrafactual_set, solution_list\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CCSSE:\n",
    "    def __init__(self, dataset, bb_model, samples = None, K = 5, generation = 10, dataset_size = 'all'):\n",
    "        self.df_datasets = self.load_df_dataset()\n",
    "        self.dataset = dataset\n",
    "        self.samples = samples\n",
    "        self.K = K\n",
    "        self.generation = generation\n",
    "        \n",
    "#         self.x_train, self.x_test, self.y_train, self.y_test, self.dfx_full, self.dfy_full = self.get_datasets_train_test()\n",
    "        self.x_train, self.x_test, self.y_train, self.y_test, self.dfx_full, self.dfy_full = self.get_dataset()\n",
    "\n",
    "        self.bb_model, self.p = self.get_bb_model(bb_model)\n",
    "        self.explainerCSSE = self.get_model_contrafactual()\n",
    "        self.model_causal, self.df_causal_effects, self.df_error, self.causal_order = self.get_model_causality()\n",
    "    \n",
    "        self.run_dict = {}\n",
    "        self.run_non_causal_dict = {}\n",
    "        \n",
    "    def load_df_dataset(self):\n",
    "        def convert_to_list(val):\n",
    "            try:\n",
    "                return ast.literal_eval(val) if isinstance(val, str) and val.startswith('[') and val.endswith(']') else val\n",
    "            except (ValueError, SyntaxError):\n",
    "                return val\n",
    "            \n",
    "        df = pd.read_parquet(\"dfm_use.parquet\")\n",
    "        df['path'] = df['path'].apply(convert_to_list)\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n",
    "    def get_dataset(self):\n",
    "        dataset_dict = self.df_datasets[self.df_datasets['name'] == self.dataset].iloc[0].to_dict()\n",
    "        \n",
    "        if isinstance(dataset_dict['path'], list):\n",
    "            if 'Column' in dataset_dict['classe']:\n",
    "                df_train = pd.read_csv(f\"datasets/drive_raw/{dataset_dict['path'][0]}\", header = None)\n",
    "                df_test = pd.read_csv(f\"datasets/drive_raw/{dataset_dict['path'][1]}\", header = None)\n",
    "                class_name = int(dataset_dict['class'].split('Column')[1]) - 1\n",
    "            else:\n",
    "                df_train = pd.read_csv(f\"datasets/drive_raw/{dataset_dict['path'][0]}\")\n",
    "                df_test = pd.read_csv(f\"datasets/drive_raw/{dataset_dict['path'][1]}\")\n",
    "                class_name = dataset_dict['classe']\n",
    "            \n",
    "            x_train = df_train.drop(columns=[class_name])\n",
    "            y_train = df_train[class_name]\n",
    "\n",
    "            # Dividindo o df_test\n",
    "            x_test = df_test.drop(columns=[class_name])\n",
    "            y_test = df_test[class_name]\n",
    "            \n",
    "            dfx_full = pd.concat([x_train, x_test])\n",
    "            dfy_full = pd.concat([y_train, y_test])\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            if 'Column' in dataset_dict['classe']:\n",
    "                df_main = pd.read_csv(f\"datasets/drive_raw/{dataset_dict['path']}\", header = None)\n",
    "                class_name = int(dataset_dict['classe'].split('Column')[1]) - 1\n",
    "            else:\n",
    "                df_main = pd.read_csv(f\"datasets/drive_raw/{dataset_dict['path']}\")\n",
    "                class_name = dataset_dict['classe']\n",
    "            \n",
    "            columns = df_main.columns\n",
    "            columns_tmp = list(columns)\n",
    "            columns_tmp.remove(class_name)\n",
    "\n",
    "            x_train, x_test, y_train, y_test = train_test_split(df_main[columns_tmp], df_main[class_name], test_size=0.1)\n",
    "\n",
    "            dfx_full = pd.concat([x_train, x_test])\n",
    "            dfy_full = pd.concat([y_train, y_test])\n",
    "            \n",
    "        return x_train, x_test, y_train, y_test, dfx_full, dfy_full\n",
    "    \n",
    "\n",
    "    def get_bb_model(self, bb_model_name):\n",
    "        \n",
    "        if bb_model_name == 'rf':\n",
    "            bb_model = RandomForestClassifier()  \n",
    "            bb_model.fit(self.x_train, self.y_train)\n",
    "\n",
    "            p = bb_model.predict(self.x_test)\n",
    "\n",
    "            print(classification_report(self.y_test, p))\n",
    "\n",
    "            return bb_model, p\n",
    "        elif bb_model_name == 'rn':\n",
    "            bb_model = MLPClassifier()  \n",
    "            bb_model.fit(self.x_train, self.y_train)\n",
    "\n",
    "            p = bb_model.predict(self.x_test)\n",
    "\n",
    "            print(classification_report(self.y_test, p))\n",
    "\n",
    "            return bb_model, p\n",
    "\n",
    "    def get_model_contrafactual(self):\n",
    "        return CSSE(self.dfx_full, self.bb_model, K = self.K, num_gen = self.generation)\n",
    "\n",
    "    def get_model_causality(self):\n",
    "        dataset_dict = self.df_datasets[self.df_datasets['name'] == self.dataset].iloc[0].to_dict()\n",
    "\n",
    "        model_causal = lingam.DirectLiNGAM()\n",
    "        model_causal.fit(self.dfx_full)\n",
    "        causal_order = [labels[x] for x in model_causal.causal_order_]\n",
    "\n",
    "        if dataset_dict['tipo'] in ['Mista', 'Categórica']:\n",
    "            model_causal = lingam.LiM()\n",
    "            model_causal.fit(self.dfx_full.values, np.array([[0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,]]), only_global=True)\n",
    "\n",
    "            \n",
    "        labels = [f'{i}' for i in self.dfx_full.columns]\n",
    "        \n",
    "        matrix = model_causal.adjacency_matrix_\n",
    "        from_list = []\n",
    "        to_list = []\n",
    "        effect_list = []\n",
    "\n",
    "        # Iteração sobre a matriz para extrair os valores e suas posições\n",
    "        for i in range(len(matrix)):\n",
    "            for j in range(len(matrix[i])):\n",
    "                if matrix[i][j] != 0:\n",
    "                    from_list.append(j)\n",
    "                    to_list.append(i)\n",
    "                    effect_list.append(matrix[i][j])\n",
    "\n",
    "        # Criando o DataFrame\n",
    "        df_causal_effects = pd.DataFrame({'from': from_list, 'to': to_list, 'effect': effect_list})\n",
    "        labels = [f'{i}' for i in self.dfx_full.columns]\n",
    "        df_causal_effects['from'] = df_causal_effects['from'].apply(lambda x : labels[x])\n",
    "        df_causal_effects['to'] = df_causal_effects['to'].apply(lambda x : labels[x])\n",
    "\n",
    "\n",
    "        matrix_error = model_causal.get_error_independence_p_values(self.dfx_full)\n",
    "        from_list = []\n",
    "        to_list = []\n",
    "        effect_list = []\n",
    "\n",
    "        # Iteração sobre a matriz para extrair os valores e suas posições\n",
    "        for i in range(len(matrix_error)):\n",
    "            for j in range(i + 1, len(matrix_error[i])):\n",
    "                if matrix_error[i][j] != 0:\n",
    "                    from_list.append(j)\n",
    "                    to_list.append(i)\n",
    "                    effect_list.append(matrix_error[i][j])\n",
    "\n",
    "        # Criando o DataFrame\n",
    "        df_error = pd.DataFrame({'from': from_list, 'to': to_list, 'effect': effect_list})\n",
    "        labels = [f'{i}' for i in self.dfx_full.columns]\n",
    "        df_error['from'] = df_error['from'].apply(lambda x : labels[x])\n",
    "        df_error['to'] = df_error['to'].apply(lambda x : labels[x])\n",
    "        df_error.fillna(0, inplace = True)\n",
    "\n",
    "        return model_causal, df_causal_effects, df_error, causal_order\n",
    "        \n",
    "    \n",
    "    def print_causal_graph(self):\n",
    "        make_dot(self.model_causal.adjacency_matrix_)\n",
    "\n",
    "    def run_non_causal(self):\n",
    "        self.run_non_causal_dict = {}\n",
    "\n",
    "        if isinstance(self.samples, list):\n",
    "            self.create_run_dict(self)\n",
    "            for sample in self.samples:\n",
    "                self.run_non_causal_sample(sample)\n",
    "                \n",
    "        elif isinstance(self.samples, int):\n",
    "            for sample in range(self.samples):\n",
    "                self.run_non_causal_sample(sample)\n",
    "        \n",
    "        else: \n",
    "            for sample in range(10):\n",
    "                self.run_non_causal_sample(sample)\n",
    "                \n",
    "    def run_non_causal_sample(self, sample):\n",
    "        self.run_non_causal_dict[sample] = {}\n",
    "        original_instance = self.x_test.iloc[sample].copy()\n",
    "        contrafactual_set, solution = self.explainerCSSE.explain(original_instance, self.p[sample]) #Method returns the list of counterfactuals and the explanations generated from them\n",
    "\n",
    "        self.run_non_causal_dict[sample]['solution'] = solution\n",
    "\n",
    "    def run_causal(self):\n",
    "        start_time = time.time()\n",
    "        self.run_dict = {}\n",
    "        self.run_dict['global_numbers'] = {\n",
    "                    \"global_quant_changes\": 0,\n",
    "                    \"global_quant_causal_changes\": 0,\n",
    "                    \"global_quant_causal_rules\": 0,\n",
    "                    \"global_quant_zeros_causal\": 0,\n",
    "                    \"global_quant_full_causal\": 0,\n",
    "                    \"global_quant_causal_contrafac\": 0,\n",
    "                    \"global_quant_maioria_causal_satisfeita\": 0,\n",
    "                    \"global_quant_contrafac_unico\": 0,\n",
    "            }\n",
    "        self.global_quant_contrafac_max = 0\n",
    "        if isinstance(self.samples, list):\n",
    "            for sample in self.samples:\n",
    "                self.run_causal_sample(sample)\n",
    "                \n",
    "        elif isinstance(self.samples, int):\n",
    "            for sample in range(self.samples):\n",
    "                try:\n",
    "                    self.run_causal_sample(sample)\n",
    "                except Exception as e:\n",
    "                    print(f\"DEBUG ERRO: {e}\")\n",
    "        \n",
    "        else: \n",
    "            for sample in range(10):\n",
    "                self.run_causal_sample(sample)\n",
    "        \n",
    "        self.global_quant_contrafac_max = self.K * len(self.run_dict)\n",
    "        self.run_dict['global_numbers']['global_timing_run_causal'] = time.time() - start_time\n",
    "\n",
    "\n",
    "    def run_causal_sample(self, sample):\n",
    "        if isinstance(self.samples, list):\n",
    "            original_instance = self.dfx_full.iloc[sample]\n",
    "        else:\n",
    "            original_instance = self.x_test.iloc[sample]\n",
    "        self.run_dict[sample] = {}\n",
    "        self.run_dict[sample]['original_instance'] = original_instance\n",
    "\n",
    "#         print(f'Running original instance:\\n {display(original_instance)}')\n",
    "        print(f'Start to Running samples')\n",
    "\n",
    "        causal_explain = self.get_causal_explain(sample)\n",
    "        self.run_dict[sample]['causal_explain'] = causal_explain\n",
    "\n",
    "        list_analyse = []\n",
    "        for contrafactual in causal_explain[0]:\n",
    "            list_analyse.append(self.analyse_contrafac(contrafactual, causal_explain[1], causal_explain[2]))\n",
    "\n",
    "        self.run_dict[sample]['list_analyse'] = list_analyse\n",
    "        self.analyse_explaination(sample)\n",
    "\n",
    "    def analyse_contrafac(self, contrafac, df, original_ind):\n",
    "        columns = [x.column for x in contrafac]\n",
    "        condicao = (df['to'].isin(columns)) & (df['from'].isin(columns))\n",
    "        ind = original_ind[columns]\n",
    "        return [contrafac, df[condicao], ind]\n",
    "\n",
    "    def get_causal_explain(self, sample):\n",
    "        if isinstance(self.samples, list):\n",
    "            original_ind = self.dfx_full.iloc[sample].copy()\n",
    "        else:\n",
    "            original_ind = self.x_test.iloc[sample].copy() #Original instance\n",
    "        #self.ind_cur_class = ind_cur_class #Index in the shap corresponds to the original instance class\n",
    "        self.explainerCSSE.current_class = self.p[sample] #Original instance class\n",
    "        self.explainerCSSE.original_ind = original_ind\n",
    "        \n",
    "        ind_cur_class = self.explainerCSSE.getBadClass()\n",
    "\n",
    "        #Gets the valid values range of each feature\n",
    "        features_range = []\n",
    "        features_range = self.explainerCSSE.getFeaturesRange()\n",
    "\n",
    "        #The DataFrame df will have the current population\n",
    "        df = pd.DataFrame(columns=self.explainerCSSE.input_dataset.columns)\n",
    "\n",
    "        #Generates the initial population with popinitial mutants        \n",
    "        self.explainerCSSE.getPopInicial(df, features_range)\n",
    "        df_causal = df.copy()\n",
    "        dict_dfs = {}\n",
    "\n",
    "        # for g in tqdm(range(self.explainerCSSE.num_gen), desc= \"Processing...\"):\n",
    "        for g in range(self.generation):\n",
    "\n",
    "            #To use on the parents of each generation\n",
    "            old_parents = pd.DataFrame(columns=self.explainerCSSE.input_dataset.columns)\n",
    "\n",
    "            #Copy parents to the next generation\n",
    "            old_parents = df_causal.copy()\n",
    "            dict_dfs[g] = {}\n",
    "\n",
    "            parents_causal = self.apply_causality(old_parents)\n",
    "            dict_dfs[g]['causal_parents'] = parents_causal\n",
    "            #df will contain the new population\n",
    "            df_causal = pd.DataFrame(columns=self.explainerCSSE.input_dataset.columns)\n",
    "            evaluation_causal = []\n",
    "\n",
    "            #Assessing generation counterfactuals\n",
    "            self.explainerCSSE.fitness(dict_dfs[g]['causal_parents'], evaluation_causal, ind_cur_class)\n",
    "\n",
    "            #The original individual will always be in the 0 position of the df - So that it is normalized too (it will be used later in the distance function)\n",
    "            df_causal.loc[0] = original_ind.copy()\n",
    "\n",
    "            #Copies to the next generation the per_elit best individuals\n",
    "            self.explainerCSSE.elitism(evaluation_causal, df_causal, parents_causal)\n",
    "            number_cross_repetitions = 0\n",
    "            while len(df_causal) < self.explainerCSSE.pop_size + 1: #+1, as the 1st position is used to store the reference individual\n",
    "                number_cross_repetitions_causal = self.explainerCSSE.crossover(df_causal, parents_causal, evaluation_causal, number_cross_repetitions)\n",
    "\n",
    "                mutation_op = rnd.random()\n",
    "                if mutation_op <= self.explainerCSSE.mutation_proba:\n",
    "                    self.explainerCSSE.mutation(df_causal, len(df_causal) - 1, features_range)\n",
    "\n",
    "\n",
    "        evaluation = []\n",
    "        evaluation_causal = []\n",
    "\n",
    "        #Evaluating the latest generation\n",
    "        self.explainerCSSE.fitness(df_causal, evaluation_causal, ind_cur_class)\n",
    "\n",
    "        #Order the last generation by distance to the original instance     \n",
    "        evaluation_causal.sort(key=lambda individual: individual.aval_norm) \n",
    "\n",
    "        #Getting the counterfactual CAUSAL set\n",
    "        contrafactual_set_causal, solution_list_causal = self.explainerCSSE.getContrafactual(df_causal, evaluation_causal) \n",
    "\n",
    "        dict_dfs['contrafactual_set_causal'] = contrafactual_set_causal\n",
    "        dict_dfs['solution_list_causal'] = solution_list_causal\n",
    "        \n",
    "        df_contrafac_causal = self.get_contrafac_df_causal(solution_list_causal)\n",
    "        return [solution_list_causal, df_contrafac_causal, original_ind]\n",
    "    \n",
    "\n",
    "    def apply_causality(self, df):\n",
    "        df_apply_causal = pd.DataFrame(columns = df.columns)\n",
    "        original = df.iloc[0]\n",
    "        df_apply_causal.loc[0] = original\n",
    "        for index, df_row in df.iloc[1:].iterrows():\n",
    "            causal_ind = df_row.copy()\n",
    "            for column in self.causal_order:\n",
    "                value_diff = causal_ind[column] - original[column]\n",
    "                if value_diff != 0:\n",
    "                    tmp_effects = self.df_causal_effects[self.df_causal_effects['from'] == column]\n",
    "                    for index, row in tmp_effects.iterrows():\n",
    "    #                     prob = rnd.random()\n",
    "    #                     if row['probability'] <= prob:\n",
    "                        tmp_error = self.df_error[self.df_error['from'].isin([column, row['to']]) | self.df_error['to'].isin([column, row['to']])]\n",
    "                        error_value = tmp_error['effect'].iloc[0]\n",
    "    #                     print(f'error value = {error_value}')\n",
    "                        causal_ind[row['to']] = causal_ind[row['to']] + (value_diff * row['effect']) + tmp_error['effect'].iloc[0]\n",
    "            df_apply_causal.loc[len(df_apply_causal)] = causal_ind\n",
    "        return df_apply_causal\n",
    "\n",
    "\n",
    "    def get_contrafac_df_causal(self, solution_list_causal):\n",
    "        lista_solution_causal = [[t.column for t in sublist] for sublist in solution_list_causal]\n",
    "\n",
    "        # Inicializa uma lista para armazenar os resultados\n",
    "        resultados = []\n",
    "\n",
    "        # Loop sobre os valores na lista\n",
    "        for lista_valores in lista_solution_causal:\n",
    "            if len(lista_valores) > 1:\n",
    "                for v1 in lista_valores:\n",
    "                    for v2 in lista_valores:\n",
    "                        if v1 != v2:\n",
    "                            # Cria uma condição para cada par de valores diferentes na lista\n",
    "                            condicao = (self.df_causal_effects['to'].isin([v1, v2])) & (self.df_causal_effects['from'].isin([v1, v2]))\n",
    "                            # Realiza a busca no DataFrame usando a condição e armazena os resultados\n",
    "                            resultados.append(self.df_causal_effects[condicao])\n",
    "\n",
    "        # Concatena os resultados em um único DataFrame\n",
    "        if resultados:\n",
    "            resultado_final = pd.concat(resultados)\n",
    "            resultado_final = resultado_final.drop_duplicates()\n",
    "        else:\n",
    "            resultado_final = pd.DataFrame(columns = self.df_causal_effects.columns)\n",
    "            \n",
    "        return resultado_final\n",
    "    \n",
    "\n",
    "    def analyse_explaination(self, sample):\n",
    "        self.run_dict[sample]['data_analysis'] = []\n",
    "        for i, content in enumerate(self.run_dict[sample]['list_analyse']):\n",
    "            self.global_quant_contrafac_max += 1\n",
    "            controle = {}\n",
    "            causal = content[0]\n",
    "            df = content[1]\n",
    "            ori = content[2]\n",
    "            \n",
    "            \n",
    "            num_changes = len(causal)\n",
    "            self.run_dict['global_numbers']['global_quant_changes'] += num_changes\n",
    "            \n",
    "            num_causal_rules = len(df)\n",
    "            self.run_dict['global_numbers']['global_quant_causal_rules'] += num_causal_rules\n",
    "            \n",
    "            for attr in causal:\n",
    "                key = attr.column\n",
    "                if attr.value > ori[key]:\n",
    "                    controle[key] = 'mais'\n",
    "                else:\n",
    "                    controle[key] = 'menos'\n",
    "\n",
    "            df_temp = df.copy()\n",
    "            df_temp['from'] = df['from'].map(controle)\n",
    "            df_temp['to'] = df['to'].map(controle)\n",
    "            if len(df_temp) > 0:\n",
    "                df_temp['causal'] = df_temp.apply(lambda row: self.verificar_condicoes(row), axis = 1)\n",
    "                causal_finds = df_temp['causal'].sum()\n",
    "            else:\n",
    "                causal_finds = 0\n",
    "                \n",
    "            data_dict = {}\n",
    "\n",
    "            data_dict['df_respeita_causal'] = df_temp\n",
    "            data_dict['contrafactual_causal'] = causal\n",
    "            data_dict['df_causal_effects'] = df\n",
    "            \n",
    "            self.run_dict[sample]['data_analysis'].append(data_dict)\n",
    "\n",
    "            self.run_dict['global_numbers']['global_quant_causal_changes'] += causal_finds\n",
    "            \n",
    "            # print(f'causal = \\n{causal}\\n')\n",
    "            # print(f'original = \\n{ori}\\n')\n",
    "            # print(f'df_temp = \\n{display(df_temp)}\\n')\n",
    "            \n",
    "            if len(df_temp) > 0:\n",
    "                if causal_finds > 0:\n",
    "                    self.run_dict['global_numbers']['global_quant_causal_contrafac'] += 1\n",
    "                else:\n",
    "                    # print(f'nenhuma relaçao causal satisfeita')\n",
    "                    self.run_dict['global_numbers']['global_quant_zeros_causal'] += 1\n",
    "    #                 display(df_temp)\n",
    "    #                 print(f\"original = {ori}\")\n",
    "    #                 print(f\"causal = {causal}\")\n",
    "\n",
    "                if causal_finds == num_causal_rules:\n",
    "                    self.run_dict['global_numbers']['global_quant_full_causal'] += 1\n",
    "                    # if causal_finds > 2:\n",
    "                        # print(f'todas > 2 relaçoes causais satisfeitas')\n",
    "    #                     display(df_temp)\n",
    "    #                     print(f\"original = {ori}\")\n",
    "    #                     print(f\"causal = {causal}\")\n",
    "                    # elif causal_finds == 1:\n",
    "                        # print(f'todas = 1 relaçoes causais satisfeitas')\n",
    "                \n",
    "                if causal_finds >= (len(df_temp)/2):\n",
    "                    self.run_dict['global_numbers']['global_quant_maioria_causal_satisfeita'] += 1\n",
    "            else:\n",
    "    #             if len(causal) > 0:\n",
    "                self.run_dict['global_numbers']['global_quant_contrafac_unico'] += 1\n",
    "        \n",
    "    def verificar_condicoes(self, row):\n",
    "        if (row['from'] == 'mais' and row['to'] == 'mais' and row['effect'] > 0):\n",
    "            return True\n",
    "        elif row['from'] == 'menos' and row['to'] == 'menos' and row['effect'] > 0:\n",
    "            return True\n",
    "        elif row['from'] == 'mais' and row['to'] == 'menos' and row['effect'] < 0:\n",
    "            return True\n",
    "        elif row['from'] == 'menos' and row['to'] == 'mais' and row['effect'] < 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "            \n",
    "\n",
    "    def show_metrics(self, get_output = False):\n",
    "        \n",
    "        print(f\"Quantidade de instâncias contrafactuais = {self.global_quant_contrafac_max}\")\n",
    "        print(f'Quantidade de relações causais na base de dados = {len(self.df_causal_effects)}')\n",
    "        print(f\"Quantidade de atributos modificados = {self.run_dict['global_numbers']['global_quant_changes']}\")\n",
    "        print(f\"Quantidade de instâncias contrafactuais causais = {self.run_dict['global_numbers']['global_quant_contrafac_unico'] + self.run_dict['global_numbers']['global_quant_causal_contrafac']}\")\n",
    "        print(f\"Quantidade de relações causais analisadas = {self.run_dict['global_numbers']['global_quant_causal_rules']}\")\n",
    "        print(f\"Quantidade de relações causais satisfeitas = {self.run_dict['global_numbers']['global_quant_causal_changes']}\")\n",
    "        print(f\"Quantidade de instâncias contrafactuais com um único atributo modificado = {self.run_dict['global_numbers']['global_quant_contrafac_unico']}\")\n",
    "        print(f\"Tempo de execução = {self.run_dict['global_numbers']['global_timing_run_causal']}\")\n",
    "        \n",
    "        if get_output:\n",
    "            metrics_dict = {\n",
    "                \"Quantidade de instâncias contrafactuais\": self.global_quant_contrafac_max,\n",
    "                \"Quantidade de relações causais na base de dados\": len(self.df_causal_effects),\n",
    "                \"Quantidade de atributos modificados\": self.run_dict['global_numbers']['global_quant_changes'],\n",
    "                \"Quantidade de instâncias contrafactuais causais\": self.run_dict['global_numbers']['global_quant_contrafac_unico'] + self.run_dict['global_numbers']['global_quant_causal_contrafac'],\n",
    "                \"Quantidade de relações causais analisadas\": self.run_dict['global_numbers']['global_quant_causal_rules'],\n",
    "                \"Quantidade de relações causais satisfeitas\": self.run_dict['global_numbers']['global_quant_causal_changes'],\n",
    "                \"Quantidade de instâncias contrafactuais com um único atributo modificado\": self.run_dict['global_numbers']['global_quant_contrafac_unico'],\n",
    "                \"Tempo de execução\": self.run_dict['global_numbers']['global_timing_run_causal']\n",
    "            }\n",
    "        \n",
    "            return metrics_dict\n",
    "\n",
    "def get_causal_metrics(row, bb_model_name):\n",
    "#     try:\n",
    "    print(row)\n",
    "    ccsse = CCSSE(row['name'], samples = 10, K = 10, generation= 10, bb_model = bb_model_name)\n",
    "    print(f\"criou o modelo\")\n",
    "    ccsse.run_causal()\n",
    "    print(f\"run_causal completo\")\n",
    "    dict_metricas = ccsse.show_metrics(get_output = True)\n",
    "    converted_dict_metricas = convert_np_types(dict_metricas)\n",
    "    json_data = json.dumps(converted_dict_metricas, indent=4)\n",
    "\n",
    "    s3.put_object(Bucket='omar-testes-gerais', Key=f'artigos/causal_csse/bateria_metricas/outputs/metricas_manuais/{bb_model_name}/{row[\"name\"]}.json', Body=json_data)\n",
    "    print(f\"Execução completa para {row['name']}\")\n",
    "    return json_data\n",
    "#     except Exception as e:\n",
    "#         print(f'Execução falhou - Nome da base de dados: {row[\"name\"]}')\n",
    "#         print(e)\n",
    "\n",
    "def convert_np_types(data):\n",
    "    \"\"\"Converte tipos de dados NumPy em tipos nativos do Python.\"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        return {key: convert_np_types(value) for key, value in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [convert_np_types(item) for item in data]\n",
    "    elif isinstance(data, np.int64):\n",
    "        return int(data)  # Converte int64 para int\n",
    "    elif isinstance(data, np.float64):\n",
    "        return float(data)  # Converte float64 para float\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "#PJ LOCAL\n",
    "def handler(dataset, model_name):\n",
    "    \n",
    "    args = {\n",
    "        \"list_dataset_name\": dataset,\n",
    "        'bb_model_name': model_name\n",
    "    }\n",
    "    \n",
    "    dfm_use = pd.read_parquet(f\"dfm_use.parquet\")\n",
    "#     df_dataset = df_map_inference_datasets[df_map_inference_datasets['name'].isin(args[\"list_dataset_name\"])]\n",
    "\n",
    "    for idx, row in dfm_use.iterrows():\n",
    "        if row['name'] in args['list_dataset_name']:\n",
    "            json_response = get_causal_metrics(row, args[\"bb_model_name\"])\n",
    "            dfm_use.loc[idx, 'metrica'] = json_response\n",
    "#     dfm_use = dfm_use['name'].isin(args[\"list_dataset_name\"]).apply(lambda x: get_causal_metrics(x, args[\"bb_model_name\"]), axis = 1)\n",
    "#     df_map_inference_datasets.to_parquet(f\"s3://omar-testes-gerais/artigos/artifacts/dfm_use.parquet\", engine = 'pyarrow')\n",
    "    \n",
    "    return dfm_use\n",
    "\n",
    "#PJ REAL\n",
    "# if __name__ == '__main__':\n",
    "    \n",
    "#     parser = argparse.ArgumentParser()\n",
    "    \n",
    "#     # Adicionando os argumentos para input e output\n",
    "#     parser.add_argument('--list_dataset_name', type=str)\n",
    "#     parser.add_argument('--bb_model_name', type=str)\n",
    "    \n",
    "#     args = parser.parse_args()\n",
    "#     list_dataset_name = ast.literal_eval(args.list_dataset_name)\n",
    "\n",
    "#     df_map_inference_datasets = pd.read_parquet(f\"s3://omar-testes-gerais/artigos/artifacts/df_map_inference_datasets.parquet\")\n",
    "#     df_dataset = df_map_inference_datasets[df_map_inference_datasets['name'].isin(list_dataset_name)]\n",
    "#     df_dataset.apply(lambda x: get_causal_metrics(x, args.bb_model_name), axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "352ecf04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>path</th>\n",
       "      <th>classe</th>\n",
       "      <th>metrica</th>\n",
       "      <th>metrica_v2</th>\n",
       "      <th>gerou_resultado</th>\n",
       "      <th>tamanho</th>\n",
       "      <th>tipo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Australian</td>\n",
       "      <td>Australian/Credit_Card_Applications.csv</td>\n",
       "      <td>Class</td>\n",
       "      <td>{\"Quantidade de inst\\u00e2ncias contrafactuais...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>Média</td>\n",
       "      <td>Mista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Banknote</td>\n",
       "      <td>Banknote/BankNote_Authentication.csv</td>\n",
       "      <td>class</td>\n",
       "      <td>{\"Quantidade de inst\\u00e2ncias contrafactuais...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>Pequena</td>\n",
       "      <td>Numérica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Biodeg</td>\n",
       "      <td>Biodeg/qsar-biodeg.csv</td>\n",
       "      <td>Class</td>\n",
       "      <td>{\\n    \"Quantidade de instâncias contrafactuai...</td>\n",
       "      <td>zero metricas</td>\n",
       "      <td>1</td>\n",
       "      <td>Grande</td>\n",
       "      <td>Numérica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Breast Cancer Coimbra</td>\n",
       "      <td>Breast Cancer Coimbra/breast_coimbra.csv</td>\n",
       "      <td>Classification</td>\n",
       "      <td>{\"Quantidade de inst\\u00e2ncias contrafactuais...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>Pequena</td>\n",
       "      <td>Numérica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Breast Cancer Wisconsin</td>\n",
       "      <td>Breast Cancer Wisconsin/breast-cancer.csv</td>\n",
       "      <td>diagnosis</td>\n",
       "      <td>{\"Quantidade de inst\\u00e2ncias contrafactuais...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>Grande</td>\n",
       "      <td>Numérica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      name                                       path  \\\n",
       "1               Australian    Australian/Credit_Card_Applications.csv   \n",
       "2                 Banknote       Banknote/BankNote_Authentication.csv   \n",
       "3                   Biodeg                     Biodeg/qsar-biodeg.csv   \n",
       "4    Breast Cancer Coimbra   Breast Cancer Coimbra/breast_coimbra.csv   \n",
       "5  Breast Cancer Wisconsin  Breast Cancer Wisconsin/breast-cancer.csv   \n",
       "\n",
       "           classe                                            metrica  \\\n",
       "1           Class  {\"Quantidade de inst\\u00e2ncias contrafactuais...   \n",
       "2           class  {\"Quantidade de inst\\u00e2ncias contrafactuais...   \n",
       "3           Class  {\\n    \"Quantidade de instâncias contrafactuai...   \n",
       "4  Classification  {\"Quantidade de inst\\u00e2ncias contrafactuais...   \n",
       "5       diagnosis  {\"Quantidade de inst\\u00e2ncias contrafactuais...   \n",
       "\n",
       "      metrica_v2 gerou_resultado  tamanho      tipo  \n",
       "1           None               1    Média     Mista  \n",
       "2           None               1  Pequena  Numérica  \n",
       "3  zero metricas               1   Grande  Numérica  \n",
       "4           None               1  Pequena  Numérica  \n",
       "5           None               1   Grande  Numérica  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_mapping = {\n",
    "    'Banknote': {'Tamanho': 'Pequena', 'Tipo': 'Numérica'},\n",
    "    'mammographic_masses': {'Tamanho': 'Pequena', 'Tipo': 'Categórica'},\n",
    "    'Room Occupancy': {'Tamanho': 'Pequena', 'Tipo': 'Numérica'},\n",
    "    'liver disorders_bupa': {'Tamanho': 'Pequena', 'Tipo': 'Numérica'},\n",
    "    'Monk_1': {'Tamanho': 'Pequena', 'Tipo': 'Categórica'},\n",
    "    'Monk_2': {'Tamanho': 'Pequena', 'Tipo': 'Categórica'},\n",
    "    'Monk_3': {'Tamanho': 'Pequena', 'Tipo': 'Categórica'},\n",
    "    'Vertebral_2C': {'Tamanho': 'Pequena', 'Tipo': 'Numérica'},\n",
    "    'Diabetes': {'Tamanho': 'Pequena', 'Tipo': 'Numérica'},\n",
    "    'Breast Cancer Coimbra': {'Tamanho': 'Pequena', 'Tipo': 'Numérica'},\n",
    "    'Heart': {'Tamanho': 'Média', 'Tipo': 'Mista'},\n",
    "    'Heart_2': {'Tamanho': 'Média', 'Tipo': 'Mista'},\n",
    "    'EEg': {'Tamanho': 'Média', 'Tipo': 'Numérica'},\n",
    "    'Australian': {'Tamanho': 'Média', 'Tipo': 'Mista'},\n",
    "    'Votes_Congressional': {'Tamanho': 'Média', 'Tipo': 'Categórica'},\n",
    "    'German': {'Tamanho': 'Média', 'Tipo': 'Mista'},\n",
    "    'Mobile Price Classification': {'Tamanho': 'Média', 'Tipo': 'Numérica'},\n",
    "    'twonorm': {'Tamanho': 'Média', 'Tipo': 'Numérica'},\n",
    "    'KC2': {'Tamanho': 'Média', 'Tipo': 'Numérica'},\n",
    "    'HELOC': {'Tamanho': 'Média', 'Tipo': 'Numérica'},\n",
    "    'titanic': {'Tamanho': 'Média', 'Tipo': 'Mista'},\n",
    "    'Phishing': {'Tamanho': 'Grande', 'Tipo': 'Categórica'},\n",
    "    'Breast Cancer Wisconsin': {'Tamanho': 'Grande', 'Tipo': 'Numérica'},\n",
    "    'Ionosfera': {'Tamanho': 'Grande', 'Tipo': 'Numérica'},\n",
    "    'Horse colic': {'Tamanho': 'Grande', 'Tipo': 'Categórica'},\n",
    "    'Churn': {'Tamanho': 'Grande', 'Tipo': 'Categórica'},\n",
    "    'Biodeg': {'Tamanho': 'Grande', 'Tipo': 'Numérica'},\n",
    "    'Student': {'Tamanho': 'Grande', 'Tipo': 'Mista'},\n",
    "    'Tokyo': {'Tamanho': 'Grande', 'Tipo': 'Numérica'},\n",
    "    'Spambase': {'Tamanho': 'Grande', 'Tipo': 'Numérica'},\n",
    "    'Sonar': {'Tamanho': 'Grande', 'Tipo': 'Numérica'},\n",
    "    'Musk': {'Tamanho': 'Grande', 'Tipo': 'Numérica'}\n",
    "}\n",
    "\n",
    "dfm_use = pd.read_parquet(f\"dfm_use.parquet\")\n",
    "dfm_use = dfm_use[dfm_use['name'].isin(dataset_mapping.keys())]\n",
    "dfm_use\n",
    "\n",
    "dfm_use['tamanho'] = dfm_use['name'].map(lambda x: dataset_mapping[x]['Tamanho'])\n",
    "dfm_use['tipo'] = dfm_use['name'].map(lambda x: dataset_mapping[x]['Tipo'])\n",
    "dfm_use.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b9cdce0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>path</th>\n",
       "      <th>classe</th>\n",
       "      <th>metrica</th>\n",
       "      <th>metrica_v2</th>\n",
       "      <th>gerou_resultado</th>\n",
       "      <th>tamanho</th>\n",
       "      <th>tipo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Australian</td>\n",
       "      <td>Australian/Credit_Card_Applications.csv</td>\n",
       "      <td>Class</td>\n",
       "      <td>{\"Quantidade de inst\\u00e2ncias contrafactuais...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>Média</td>\n",
       "      <td>Mista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Banknote</td>\n",
       "      <td>Banknote/BankNote_Authentication.csv</td>\n",
       "      <td>class</td>\n",
       "      <td>{\"Quantidade de inst\\u00e2ncias contrafactuais...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>Pequena</td>\n",
       "      <td>Numérica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Biodeg</td>\n",
       "      <td>Biodeg/qsar-biodeg.csv</td>\n",
       "      <td>Class</td>\n",
       "      <td>{\\n    \"Quantidade de instâncias contrafactuai...</td>\n",
       "      <td>zero metricas</td>\n",
       "      <td>1</td>\n",
       "      <td>Grande</td>\n",
       "      <td>Numérica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Breast Cancer Coimbra</td>\n",
       "      <td>Breast Cancer Coimbra/breast_coimbra.csv</td>\n",
       "      <td>Classification</td>\n",
       "      <td>{\"Quantidade de inst\\u00e2ncias contrafactuais...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>Pequena</td>\n",
       "      <td>Numérica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Breast Cancer Wisconsin</td>\n",
       "      <td>Breast Cancer Wisconsin/breast-cancer.csv</td>\n",
       "      <td>diagnosis</td>\n",
       "      <td>{\"Quantidade de inst\\u00e2ncias contrafactuais...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>Grande</td>\n",
       "      <td>Numérica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Churn</td>\n",
       "      <td>Churn/WA_Fn-UseC_-Telco-Customer-Churn_process...</td>\n",
       "      <td>Churn</td>\n",
       "      <td>Input contains NaN, infinity or a value too la...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>Grande</td>\n",
       "      <td>Categórica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Diabetes</td>\n",
       "      <td>Diabetes/diabetes.csv</td>\n",
       "      <td>Outcome</td>\n",
       "      <td>{\"Quantidade de inst\\u00e2ncias contrafactuais...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>Pequena</td>\n",
       "      <td>Numérica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>EEg</td>\n",
       "      <td>EEg/EEG Eye State.csv</td>\n",
       "      <td>Column15</td>\n",
       "      <td>memoria</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>Média</td>\n",
       "      <td>Numérica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>German</td>\n",
       "      <td>German/german_credit.csv</td>\n",
       "      <td>default</td>\n",
       "      <td>{\"Quantidade de inst\\u00e2ncias contrafactuais...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>Média</td>\n",
       "      <td>Mista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Heart</td>\n",
       "      <td>Heart/heart.csv</td>\n",
       "      <td>output</td>\n",
       "      <td>{\"Quantidade de inst\\u00e2ncias contrafactuais...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>Média</td>\n",
       "      <td>Mista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Heart_2</td>\n",
       "      <td>Heart/heart2.csv</td>\n",
       "      <td>num</td>\n",
       "      <td>{\"Quantidade de inst\\u00e2ncias contrafactuais...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>Média</td>\n",
       "      <td>Mista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>HELOC</td>\n",
       "      <td>HELOC/heloc_dataset_v1.csv</td>\n",
       "      <td>RiskPerformance</td>\n",
       "      <td>{\"Quantidade de inst\\u00e2ncias contrafactuais...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>Média</td>\n",
       "      <td>Numérica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Horse colic</td>\n",
       "      <td>Horse colic/horseV2_processada.csv</td>\n",
       "      <td>surgery</td>\n",
       "      <td>array must not contain infs or NaNs</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>Grande</td>\n",
       "      <td>Categórica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Ionosfera</td>\n",
       "      <td>Ionosfera/ionosphere.csv</td>\n",
       "      <td>target</td>\n",
       "      <td>array must not contain infs or NaNs</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>Grande</td>\n",
       "      <td>Numérica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>KC2</td>\n",
       "      <td>KC2/KC2.csv</td>\n",
       "      <td>defects</td>\n",
       "      <td>{\"Quantidade de inst\\u00e2ncias contrafactuais...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>Média</td>\n",
       "      <td>Numérica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>liver disorders_bupa</td>\n",
       "      <td>liver disorders_bupa/bupa.csv</td>\n",
       "      <td>selector</td>\n",
       "      <td>{\"Quantidade de inst\\u00e2ncias contrafactuais...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>Pequena</td>\n",
       "      <td>Numérica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>mammographic_masses</td>\n",
       "      <td>mammographic_masses/mammographic_masses_cleane...</td>\n",
       "      <td>Severity</td>\n",
       "      <td>{\"Quantidade de inst\\u00e2ncias contrafactuais...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>Pequena</td>\n",
       "      <td>Categórica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Mobile Price Classification</td>\n",
       "      <td>Mobile Price Classification/train_mobile_proce...</td>\n",
       "      <td>range</td>\n",
       "      <td>{\\n    \"Quantidade de instâncias contrafactuai...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>Média</td>\n",
       "      <td>Numérica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Monk_1</td>\n",
       "      <td>['Monk/monks-1_train.csv', 'Monk/monks-1_test....</td>\n",
       "      <td>Class</td>\n",
       "      <td>{\"Quantidade de inst\\u00e2ncias contrafactuais...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>Pequena</td>\n",
       "      <td>Categórica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Monk_2</td>\n",
       "      <td>['Monk/monks-2_train.csv', 'Monk/monks-2_test....</td>\n",
       "      <td>Class</td>\n",
       "      <td>{\"Quantidade de inst\\u00e2ncias contrafactuais...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>Pequena</td>\n",
       "      <td>Categórica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Monk_3</td>\n",
       "      <td>['Monk/monks-3_train.csv', 'Monk/monks-3_test....</td>\n",
       "      <td>Class</td>\n",
       "      <td>{\"Quantidade de inst\\u00e2ncias contrafactuais...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>Pequena</td>\n",
       "      <td>Categórica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Musk</td>\n",
       "      <td>Musk/clean1.csv</td>\n",
       "      <td>Column165</td>\n",
       "      <td>fica dando erro de coluna</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>Grande</td>\n",
       "      <td>Numérica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Phishing</td>\n",
       "      <td>Phishing/Phishing.csv</td>\n",
       "      <td>Class</td>\n",
       "      <td>{\"Quantidade de inst\\u00e2ncias contrafactuais...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>Grande</td>\n",
       "      <td>Categórica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Room Occupancy</td>\n",
       "      <td>Room Occupancy/Room Occupancy.csv</td>\n",
       "      <td>Occupancy</td>\n",
       "      <td>{\"Quantidade de inst\\u00e2ncias contrafactuais...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>Pequena</td>\n",
       "      <td>Numérica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Sonar</td>\n",
       "      <td>Sonar/sonar.all-data.csv</td>\n",
       "      <td>class</td>\n",
       "      <td>{\"Quantidade de inst\\u00e2ncias contrafactuais...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>Grande</td>\n",
       "      <td>Numérica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Spambase</td>\n",
       "      <td>Spambase/Spambase.csv</td>\n",
       "      <td>Class</td>\n",
       "      <td>{\"Quantidade de inst\\u00e2ncias contrafactuais...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>Grande</td>\n",
       "      <td>Numérica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Student</td>\n",
       "      <td>Student/Students-Performance-MAT_processada.csv</td>\n",
       "      <td>Class</td>\n",
       "      <td>array must not contain infs or NaNs</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>Grande</td>\n",
       "      <td>Mista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>titanic</td>\n",
       "      <td>titanic/titanic_processada.csv</td>\n",
       "      <td>Survived</td>\n",
       "      <td>Input contains NaN, infinity or a value too la...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>Média</td>\n",
       "      <td>Mista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Tokyo</td>\n",
       "      <td>Tokyo/Tokyo.csv</td>\n",
       "      <td>class</td>\n",
       "      <td>{\"Quantidade de inst\\u00e2ncias contrafactuais...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>Grande</td>\n",
       "      <td>Numérica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>twonorm</td>\n",
       "      <td>twonorm/twonorm.csv</td>\n",
       "      <td>class</td>\n",
       "      <td>{\"Quantidade de inst\\u00e2ncias contrafactuais...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>Média</td>\n",
       "      <td>Numérica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Vertebral_2C</td>\n",
       "      <td>Vertebral/column_2C.csv</td>\n",
       "      <td>class</td>\n",
       "      <td>{\"Quantidade de inst\\u00e2ncias contrafactuais...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>Pequena</td>\n",
       "      <td>Numérica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Votes_Congressional</td>\n",
       "      <td>Votes_Congressional/house-votes-84_processada.csv</td>\n",
       "      <td>Class Name</td>\n",
       "      <td>{\"Quantidade de inst\\u00e2ncias contrafactuais...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>Média</td>\n",
       "      <td>Categórica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           name  \\\n",
       "1                    Australian   \n",
       "2                      Banknote   \n",
       "3                        Biodeg   \n",
       "4         Breast Cancer Coimbra   \n",
       "5       Breast Cancer Wisconsin   \n",
       "6                         Churn   \n",
       "9                      Diabetes   \n",
       "10                          EEg   \n",
       "11                       German   \n",
       "13                        Heart   \n",
       "14                      Heart_2   \n",
       "15                        HELOC   \n",
       "16                  Horse colic   \n",
       "17                    Ionosfera   \n",
       "18                          KC2   \n",
       "19         liver disorders_bupa   \n",
       "20          mammographic_masses   \n",
       "21  Mobile Price Classification   \n",
       "22                       Monk_1   \n",
       "23                       Monk_2   \n",
       "24                       Monk_3   \n",
       "25                         Musk   \n",
       "27                     Phishing   \n",
       "28               Room Occupancy   \n",
       "29                        Sonar   \n",
       "30                     Spambase   \n",
       "31                      Student   \n",
       "32                      titanic   \n",
       "33                        Tokyo   \n",
       "34                      twonorm   \n",
       "35                 Vertebral_2C   \n",
       "37          Votes_Congressional   \n",
       "\n",
       "                                                 path           classe  \\\n",
       "1             Australian/Credit_Card_Applications.csv            Class   \n",
       "2                Banknote/BankNote_Authentication.csv            class   \n",
       "3                              Biodeg/qsar-biodeg.csv            Class   \n",
       "4            Breast Cancer Coimbra/breast_coimbra.csv   Classification   \n",
       "5           Breast Cancer Wisconsin/breast-cancer.csv        diagnosis   \n",
       "6   Churn/WA_Fn-UseC_-Telco-Customer-Churn_process...            Churn   \n",
       "9                               Diabetes/diabetes.csv          Outcome   \n",
       "10                              EEg/EEG Eye State.csv         Column15   \n",
       "11                           German/german_credit.csv          default   \n",
       "13                                    Heart/heart.csv           output   \n",
       "14                                   Heart/heart2.csv              num   \n",
       "15                         HELOC/heloc_dataset_v1.csv  RiskPerformance   \n",
       "16                 Horse colic/horseV2_processada.csv          surgery   \n",
       "17                           Ionosfera/ionosphere.csv           target   \n",
       "18                                        KC2/KC2.csv          defects   \n",
       "19                      liver disorders_bupa/bupa.csv         selector   \n",
       "20  mammographic_masses/mammographic_masses_cleane...         Severity   \n",
       "21  Mobile Price Classification/train_mobile_proce...            range   \n",
       "22  ['Monk/monks-1_train.csv', 'Monk/monks-1_test....            Class   \n",
       "23  ['Monk/monks-2_train.csv', 'Monk/monks-2_test....            Class   \n",
       "24  ['Monk/monks-3_train.csv', 'Monk/monks-3_test....            Class   \n",
       "25                                    Musk/clean1.csv        Column165   \n",
       "27                              Phishing/Phishing.csv            Class   \n",
       "28                  Room Occupancy/Room Occupancy.csv        Occupancy   \n",
       "29                           Sonar/sonar.all-data.csv            class   \n",
       "30                              Spambase/Spambase.csv            Class   \n",
       "31    Student/Students-Performance-MAT_processada.csv            Class   \n",
       "32                     titanic/titanic_processada.csv         Survived   \n",
       "33                                    Tokyo/Tokyo.csv            class   \n",
       "34                                twonorm/twonorm.csv            class   \n",
       "35                            Vertebral/column_2C.csv            class   \n",
       "37  Votes_Congressional/house-votes-84_processada.csv       Class Name   \n",
       "\n",
       "                                              metrica     metrica_v2  \\\n",
       "1   {\"Quantidade de inst\\u00e2ncias contrafactuais...           None   \n",
       "2   {\"Quantidade de inst\\u00e2ncias contrafactuais...           None   \n",
       "3   {\\n    \"Quantidade de instâncias contrafactuai...  zero metricas   \n",
       "4   {\"Quantidade de inst\\u00e2ncias contrafactuais...           None   \n",
       "5   {\"Quantidade de inst\\u00e2ncias contrafactuais...           None   \n",
       "6   Input contains NaN, infinity or a value too la...           None   \n",
       "9   {\"Quantidade de inst\\u00e2ncias contrafactuais...           None   \n",
       "10                                            memoria           None   \n",
       "11  {\"Quantidade de inst\\u00e2ncias contrafactuais...           None   \n",
       "13  {\"Quantidade de inst\\u00e2ncias contrafactuais...           None   \n",
       "14  {\"Quantidade de inst\\u00e2ncias contrafactuais...           None   \n",
       "15  {\"Quantidade de inst\\u00e2ncias contrafactuais...           None   \n",
       "16                array must not contain infs or NaNs           None   \n",
       "17                array must not contain infs or NaNs           None   \n",
       "18  {\"Quantidade de inst\\u00e2ncias contrafactuais...           None   \n",
       "19  {\"Quantidade de inst\\u00e2ncias contrafactuais...           None   \n",
       "20  {\"Quantidade de inst\\u00e2ncias contrafactuais...           None   \n",
       "21  {\\n    \"Quantidade de instâncias contrafactuai...           None   \n",
       "22  {\"Quantidade de inst\\u00e2ncias contrafactuais...           None   \n",
       "23  {\"Quantidade de inst\\u00e2ncias contrafactuais...           None   \n",
       "24  {\"Quantidade de inst\\u00e2ncias contrafactuais...           None   \n",
       "25                          fica dando erro de coluna           None   \n",
       "27  {\"Quantidade de inst\\u00e2ncias contrafactuais...           None   \n",
       "28  {\"Quantidade de inst\\u00e2ncias contrafactuais...           None   \n",
       "29  {\"Quantidade de inst\\u00e2ncias contrafactuais...           None   \n",
       "30  {\"Quantidade de inst\\u00e2ncias contrafactuais...           None   \n",
       "31                array must not contain infs or NaNs           None   \n",
       "32  Input contains NaN, infinity or a value too la...           None   \n",
       "33  {\"Quantidade de inst\\u00e2ncias contrafactuais...           None   \n",
       "34  {\"Quantidade de inst\\u00e2ncias contrafactuais...           None   \n",
       "35  {\"Quantidade de inst\\u00e2ncias contrafactuais...           None   \n",
       "37  {\"Quantidade de inst\\u00e2ncias contrafactuais...           None   \n",
       "\n",
       "   gerou_resultado  tamanho        tipo  \n",
       "1                1    Média       Mista  \n",
       "2                1  Pequena    Numérica  \n",
       "3                1   Grande    Numérica  \n",
       "4                1  Pequena    Numérica  \n",
       "5                1   Grande    Numérica  \n",
       "6                1   Grande  Categórica  \n",
       "9                1  Pequena    Numérica  \n",
       "10               0    Média    Numérica  \n",
       "11               1    Média       Mista  \n",
       "13               1    Média       Mista  \n",
       "14               1    Média       Mista  \n",
       "15               1    Média    Numérica  \n",
       "16               1   Grande  Categórica  \n",
       "17               1   Grande    Numérica  \n",
       "18               1    Média    Numérica  \n",
       "19               1  Pequena    Numérica  \n",
       "20               1  Pequena  Categórica  \n",
       "21               1    Média    Numérica  \n",
       "22               1  Pequena  Categórica  \n",
       "23               1  Pequena  Categórica  \n",
       "24               1  Pequena  Categórica  \n",
       "25               1   Grande    Numérica  \n",
       "27               1   Grande  Categórica  \n",
       "28               1  Pequena    Numérica  \n",
       "29               1   Grande    Numérica  \n",
       "30               1   Grande    Numérica  \n",
       "31               1   Grande       Mista  \n",
       "32               1    Média       Mista  \n",
       "33               1   Grande    Numérica  \n",
       "34               1    Média    Numérica  \n",
       "35               1  Pequena    Numérica  \n",
       "37               1    Média  Categórica  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfm_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "45981d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm_use.to_parquet(f\"dfm_use.parquet\", engine = 'pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9e24fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.000e+00, 2.208e+01, 1.146e+01, ..., 2.000e+00, 1.000e+02,\n",
       "        1.213e+03],\n",
       "       [0.000e+00, 2.267e+01, 7.000e+00, ..., 2.000e+00, 1.600e+02,\n",
       "        1.000e+00],\n",
       "       [0.000e+00, 2.958e+01, 1.750e+00, ..., 2.000e+00, 2.800e+02,\n",
       "        1.000e+00],\n",
       "       ...,\n",
       "       [0.000e+00, 1.883e+01, 9.540e+00, ..., 2.000e+00, 1.000e+02,\n",
       "        1.000e+00],\n",
       "       [0.000e+00, 2.742e+01, 1.450e+01, ..., 2.000e+00, 1.200e+02,\n",
       "        1.200e+01],\n",
       "       [1.000e+00, 4.100e+01, 4.000e-02, ..., 1.000e+00, 5.600e+02,\n",
       "        1.000e+00]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zxc = pd.read_csv(\"datasets/drive_raw/Australian/Credit_Card_Applications.csv\")\n",
    "zxc = zxc.drop(columns = ['CustomerID', 'Class'])\n",
    "zxc.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09842501",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:00<?, ?it/s]19 [00:00<00:00, 71.13it/s, Describe variable: A14]\n",
      "Summarize dataset: 100%|██████████| 87/87 [00:03<00:00, 23.00it/s, Completed]                 \n",
      "Render JSON: 100%|██████████| 1/1 [00:00<00:00, 36.33it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ARRUMANDO O PROBLEMA DE TIPO DE DADOS\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "for idx, row in dfm_use.iterrows():\n",
    "    \n",
    "    zxc = pd.read_csv(f\"datasets/drive_raw/{row['path'][0]}\")\n",
    "\n",
    "\n",
    "    profile = ProfileReport(zxc)\n",
    "    relatorio_json = profile.to_json()\n",
    "    dados_json = json.loads(relatorio_json)\n",
    "    # Criar a lista com 0 para Categorical e 1 para Numeric\n",
    "    tipo_lista = [\n",
    "        0 if info['type'] == 'Categorical' else 1\n",
    "        for info in dados_json['variables'].values()\n",
    "    ]\n",
    "    tipo_lista\n",
    "\n",
    "    model_causal = lingam.LiM()\n",
    "    model_causal.fit(zxc.values, np.array([tipo_lista]), only_global=True)\n",
    "    model_causal._adjacency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "75057e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Os resíduos são majoritariamente NÃO-GAUSSIANOS (LiNGAM pode ser aplicável).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import normaltest\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def checking_residual_non_gaussianity(df, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Testa se os resíduos das variáveis do DataFrame são gaussianos.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame com variáveis contínuas.\n",
    "        alpha (float): Nível de significância para o teste de normalidade (default = 0.05).\n",
    "    \n",
    "    Returns:\n",
    "        bool: True se os resíduos forem não-gaussianos (bom para LiNGAM), False se forem gaussianos.\n",
    "        dict: Resultados dos testes por variável (p-values).\n",
    "    \"\"\"\n",
    "    p_values = {}\n",
    "    non_gaussian_count = 0\n",
    "\n",
    "    for target_col in df.columns:\n",
    "        X = df.drop(columns=[target_col])\n",
    "        y = df[target_col]\n",
    "\n",
    "        # model = LinearRegression()\n",
    "        # model.fit(X, y)\n",
    "        # residuals = y - model.predict(X)\n",
    "\n",
    "        # Adiciona intercepto\n",
    "        X_with_const = sm.add_constant(X)\n",
    "        model = sm.OLS(y, X_with_const).fit()\n",
    "\n",
    "        # Obtém resíduos\n",
    "        residuals = model.resid\n",
    "\n",
    "        # Teste de normalidade (D’Agostino and Pearson’s test)\n",
    "        stat, p = normaltest(residuals)\n",
    "        p_values[target_col] = p\n",
    "\n",
    "        if p < alpha:\n",
    "            non_gaussian_count += 1\n",
    "\n",
    "    # Se a maioria dos resíduos for não-gaussiana, retorna True\n",
    "    majority_non_gaussian = non_gaussian_count > len(df.columns) / 2\n",
    "    return majority_non_gaussian, p_values\n",
    "\n",
    "\n",
    "result, p_vals = checking_residual_non_gaussianity(zxc)\n",
    "\n",
    "if result:\n",
    "    print(\"Os resíduos são majoritariamente NÃO-GAUSSIANOS (LiNGAM pode ser aplicável).\")\n",
    "else:\n",
    "    print(\"Os resíduos parecem majoritariamente GAUSSIANOS (LiNGAM pode não funcionar bem).\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "83b2088e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A1': 0.0,\n",
       " 'A2': 2.67492485274373e-29,\n",
       " 'A3': 2.3966177806253047e-39,\n",
       " 'A4': 7.766535638268781e-23,\n",
       " 'A5': 0.0032786758378818845,\n",
       " 'A6': 2.4556993633196637e-10,\n",
       " 'A7': 5.007290184514749e-60,\n",
       " 'A8': 1.2240886031129759e-25,\n",
       " 'A9': 3.470732964042708e-11,\n",
       " 'A10': 6.214089927048675e-207,\n",
       " 'A11': 0.0,\n",
       " 'A12': 4.495799411304313e-57,\n",
       " 'A13': 2.1177825100558946e-115,\n",
       " 'A14': 4.076430250580457e-270}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e63b153a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name                                                 Australian\n",
      "path                    Australian/Credit_Card_Applications.csv\n",
      "classe                                                    Class\n",
      "metrica       {\"Quantidade de inst\\u00e2ncias contrafactuais...\n",
      "metrica_v2                                                 None\n",
      "Name: 1, dtype: object\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      1.00      0.66        34\n",
      "           1       0.00      0.00      0.00        35\n",
      "\n",
      "    accuracy                           0.49        69\n",
      "   macro avg       0.25      0.50      0.33        69\n",
      "weighted avg       0.24      0.49      0.33        69\n",
      "\n",
      "criou o modelo\n",
      "Start to Running samples\n",
      "Start to Running samples\n",
      "Start to Running samples\n",
      "Start to Running samples\n",
      "Start to Running samples\n",
      "Start to Running samples\n",
      "Start to Running samples\n",
      "Start to Running samples\n",
      "Start to Running samples\n",
      "Start to Running samples\n",
      "run_causal completo\n",
      "Quantidade de instâncias contrafactuais = 110\n",
      "Quantidade de relações causais na base de dados = 25\n",
      "Quantidade de atributos modificados = 215\n",
      "Quantidade de instâncias contrafactuais causais = 32\n",
      "Quantidade de relações causais analisadas = 209\n",
      "Quantidade de relações causais satisfeitas = 193\n",
      "Quantidade de instâncias contrafactuais com um único atributo modificado = 0\n",
      "Tempo de execução = 158.21856141090393\n",
      "Execução completa para Australian\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>path</th>\n",
       "      <th>classe</th>\n",
       "      <th>metrica</th>\n",
       "      <th>metrica_v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adult</td>\n",
       "      <td>adult/adult_processada.csv</td>\n",
       "      <td>Above/Below 50K</td>\n",
       "      <td>memoria 18gb</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Australian</td>\n",
       "      <td>Australian/Credit_Card_Applications.csv</td>\n",
       "      <td>Class</td>\n",
       "      <td>{\\n    \"Quantidade de inst\\u00e2ncias contrafa...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Banknote</td>\n",
       "      <td>Banknote/BankNote_Authentication.csv</td>\n",
       "      <td>class</td>\n",
       "      <td>{\"Quantidade de inst\\u00e2ncias contrafactuais...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Biodeg</td>\n",
       "      <td>Biodeg/qsar-biodeg.csv</td>\n",
       "      <td>Class</td>\n",
       "      <td>{\\n    \"Quantidade de instâncias contrafactuai...</td>\n",
       "      <td>zero metricas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Breast Cancer Coimbra</td>\n",
       "      <td>Breast Cancer Coimbra/breast_coimbra.csv</td>\n",
       "      <td>Classification</td>\n",
       "      <td>{\"Quantidade de inst\\u00e2ncias contrafactuais...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Breast Cancer Wisconsin</td>\n",
       "      <td>Breast Cancer Wisconsin/breast-cancer.csv</td>\n",
       "      <td>diagnosis</td>\n",
       "      <td>{\"Quantidade de inst\\u00e2ncias contrafactuais...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Churn</td>\n",
       "      <td>Churn/WA_Fn-UseC_-Telco-Customer-Churn_process...</td>\n",
       "      <td>Churn</td>\n",
       "      <td>Input contains NaN, infinity or a value too la...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Compas</td>\n",
       "      <td>Compas/compas-scores-two-years.csv</td>\n",
       "      <td>two_year_recid</td>\n",
       "      <td>base de dados nao processada</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Credit default</td>\n",
       "      <td>Credit default/UCI_Credit_Card.csv</td>\n",
       "      <td>default.payment.next.month</td>\n",
       "      <td>memoria</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Diabetes</td>\n",
       "      <td>Diabetes/diabetes.csv</td>\n",
       "      <td>Outcome</td>\n",
       "      <td>{\"Quantidade de inst\\u00e2ncias contrafactuais...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>EEg</td>\n",
       "      <td>EEg/EEG Eye State.csv</td>\n",
       "      <td>Column15</td>\n",
       "      <td>memoria</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>German</td>\n",
       "      <td>German/german_credit.csv</td>\n",
       "      <td>default</td>\n",
       "      <td>{\"Quantidade de inst\\u00e2ncias contrafactuais...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>GiveMeSomeCredit</td>\n",
       "      <td>GiveMeSomeCredit/GiveMeSomeCredit_processada.csv</td>\n",
       "      <td>SeriousDlqin2yrs</td>\n",
       "      <td>memoria</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Heart</td>\n",
       "      <td>Heart/heart.csv</td>\n",
       "      <td>output</td>\n",
       "      <td>{\"Quantidade de inst\\u00e2ncias contrafactuais...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Heart_2</td>\n",
       "      <td>Heart/heart2.csv</td>\n",
       "      <td>num</td>\n",
       "      <td>{\"Quantidade de inst\\u00e2ncias contrafactuais...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>HELOC</td>\n",
       "      <td>HELOC/heloc_dataset_v1.csv</td>\n",
       "      <td>RiskPerformance</td>\n",
       "      <td>{\"Quantidade de inst\\u00e2ncias contrafactuais...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Horse colic</td>\n",
       "      <td>Horse colic/horseV2_processada.csv</td>\n",
       "      <td>surgery</td>\n",
       "      <td>array must not contain infs or NaNs</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Ionosfera</td>\n",
       "      <td>Ionosfera/ionosphere.csv</td>\n",
       "      <td>target</td>\n",
       "      <td>array must not contain infs or NaNs</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>KC2</td>\n",
       "      <td>KC2/KC2.csv</td>\n",
       "      <td>defects</td>\n",
       "      <td>{\"Quantidade de inst\\u00e2ncias contrafactuais...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>liver disorders_bupa</td>\n",
       "      <td>liver disorders_bupa/bupa.csv</td>\n",
       "      <td>selector</td>\n",
       "      <td>{\"Quantidade de inst\\u00e2ncias contrafactuais...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>mammographic_masses</td>\n",
       "      <td>mammographic_masses/mammographic_masses_cleane...</td>\n",
       "      <td>Severity</td>\n",
       "      <td>{\"Quantidade de inst\\u00e2ncias contrafactuais...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Mobile Price Classification</td>\n",
       "      <td>Mobile Price Classification/train_mobile_proce...</td>\n",
       "      <td>range</td>\n",
       "      <td>{\\n    \"Quantidade de instâncias contrafactuai...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Monk_1</td>\n",
       "      <td>['Monk/monks-1_train.csv', 'Monk/monks-1_test....</td>\n",
       "      <td>Class</td>\n",
       "      <td>{\"Quantidade de inst\\u00e2ncias contrafactuais...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Monk_2</td>\n",
       "      <td>['Monk/monks-2_train.csv', 'Monk/monks-2_test....</td>\n",
       "      <td>Class</td>\n",
       "      <td>{\"Quantidade de inst\\u00e2ncias contrafactuais...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Monk_3</td>\n",
       "      <td>['Monk/monks-3_train.csv', 'Monk/monks-3_test....</td>\n",
       "      <td>Class</td>\n",
       "      <td>{\"Quantidade de inst\\u00e2ncias contrafactuais...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Musk</td>\n",
       "      <td>Musk/clean1.csv</td>\n",
       "      <td>Column165</td>\n",
       "      <td>fica dando erro de coluna</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>OnlineNewsPopularity</td>\n",
       "      <td>OnlineNewsPopularity/OnlineNewsPopularity_proc...</td>\n",
       "      <td>class</td>\n",
       "      <td>memoria</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Phishing</td>\n",
       "      <td>Phishing/Phishing.csv</td>\n",
       "      <td>Class</td>\n",
       "      <td>{\"Quantidade de inst\\u00e2ncias contrafactuais...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Room Occupancy</td>\n",
       "      <td>Room Occupancy/Room Occupancy.csv</td>\n",
       "      <td>Occupancy</td>\n",
       "      <td>{\"Quantidade de inst\\u00e2ncias contrafactuais...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Sonar</td>\n",
       "      <td>Sonar/sonar.all-data.csv</td>\n",
       "      <td>class</td>\n",
       "      <td>{\"Quantidade de inst\\u00e2ncias contrafactuais...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Spambase</td>\n",
       "      <td>Spambase/Spambase.csv</td>\n",
       "      <td>Class</td>\n",
       "      <td>{\"Quantidade de inst\\u00e2ncias contrafactuais...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Student</td>\n",
       "      <td>Student/Students-Performance-MAT_processada.csv</td>\n",
       "      <td>Class</td>\n",
       "      <td>array must not contain infs or NaNs</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>titanic</td>\n",
       "      <td>titanic/titanic_processada.csv</td>\n",
       "      <td>Survived</td>\n",
       "      <td>Input contains NaN, infinity or a value too la...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Tokyo</td>\n",
       "      <td>Tokyo/Tokyo.csv</td>\n",
       "      <td>class</td>\n",
       "      <td>{\"Quantidade de inst\\u00e2ncias contrafactuais...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>twonorm</td>\n",
       "      <td>twonorm/twonorm.csv</td>\n",
       "      <td>class</td>\n",
       "      <td>{\"Quantidade de inst\\u00e2ncias contrafactuais...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Vertebral_2C</td>\n",
       "      <td>Vertebral/column_2C.csv</td>\n",
       "      <td>class</td>\n",
       "      <td>{\"Quantidade de inst\\u00e2ncias contrafactuais...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Vertebral_3C</td>\n",
       "      <td>Vertebral/column_3C.csv</td>\n",
       "      <td>class</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Votes_Congressional</td>\n",
       "      <td>Votes_Congressional/house-votes-84_processada.csv</td>\n",
       "      <td>Class Name</td>\n",
       "      <td>{\"Quantidade de inst\\u00e2ncias contrafactuais...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>wine</td>\n",
       "      <td>wine/WineQT.csv</td>\n",
       "      <td>quality</td>\n",
       "      <td>demorou demais</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Hill</td>\n",
       "      <td>['Hill/Hill_Valley_with_noise_Training.csv', '...</td>\n",
       "      <td>class</td>\n",
       "      <td>{\\n    \"Quantidade de instâncias contrafactuai...</td>\n",
       "      <td>zero metricas</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           name  \\\n",
       "0                         Adult   \n",
       "1                    Australian   \n",
       "2                      Banknote   \n",
       "3                        Biodeg   \n",
       "4         Breast Cancer Coimbra   \n",
       "5       Breast Cancer Wisconsin   \n",
       "6                         Churn   \n",
       "7                        Compas   \n",
       "8                Credit default   \n",
       "9                      Diabetes   \n",
       "10                          EEg   \n",
       "11                       German   \n",
       "12             GiveMeSomeCredit   \n",
       "13                        Heart   \n",
       "14                      Heart_2   \n",
       "15                        HELOC   \n",
       "16                  Horse colic   \n",
       "17                    Ionosfera   \n",
       "18                          KC2   \n",
       "19         liver disorders_bupa   \n",
       "20          mammographic_masses   \n",
       "21  Mobile Price Classification   \n",
       "22                       Monk_1   \n",
       "23                       Monk_2   \n",
       "24                       Monk_3   \n",
       "25                         Musk   \n",
       "26         OnlineNewsPopularity   \n",
       "27                     Phishing   \n",
       "28               Room Occupancy   \n",
       "29                        Sonar   \n",
       "30                     Spambase   \n",
       "31                      Student   \n",
       "32                      titanic   \n",
       "33                        Tokyo   \n",
       "34                      twonorm   \n",
       "35                 Vertebral_2C   \n",
       "36                 Vertebral_3C   \n",
       "37          Votes_Congressional   \n",
       "38                         wine   \n",
       "39                         Hill   \n",
       "\n",
       "                                                 path  \\\n",
       "0                          adult/adult_processada.csv   \n",
       "1             Australian/Credit_Card_Applications.csv   \n",
       "2                Banknote/BankNote_Authentication.csv   \n",
       "3                              Biodeg/qsar-biodeg.csv   \n",
       "4            Breast Cancer Coimbra/breast_coimbra.csv   \n",
       "5           Breast Cancer Wisconsin/breast-cancer.csv   \n",
       "6   Churn/WA_Fn-UseC_-Telco-Customer-Churn_process...   \n",
       "7                  Compas/compas-scores-two-years.csv   \n",
       "8                  Credit default/UCI_Credit_Card.csv   \n",
       "9                               Diabetes/diabetes.csv   \n",
       "10                              EEg/EEG Eye State.csv   \n",
       "11                           German/german_credit.csv   \n",
       "12   GiveMeSomeCredit/GiveMeSomeCredit_processada.csv   \n",
       "13                                    Heart/heart.csv   \n",
       "14                                   Heart/heart2.csv   \n",
       "15                         HELOC/heloc_dataset_v1.csv   \n",
       "16                 Horse colic/horseV2_processada.csv   \n",
       "17                           Ionosfera/ionosphere.csv   \n",
       "18                                        KC2/KC2.csv   \n",
       "19                      liver disorders_bupa/bupa.csv   \n",
       "20  mammographic_masses/mammographic_masses_cleane...   \n",
       "21  Mobile Price Classification/train_mobile_proce...   \n",
       "22  ['Monk/monks-1_train.csv', 'Monk/monks-1_test....   \n",
       "23  ['Monk/monks-2_train.csv', 'Monk/monks-2_test....   \n",
       "24  ['Monk/monks-3_train.csv', 'Monk/monks-3_test....   \n",
       "25                                    Musk/clean1.csv   \n",
       "26  OnlineNewsPopularity/OnlineNewsPopularity_proc...   \n",
       "27                              Phishing/Phishing.csv   \n",
       "28                  Room Occupancy/Room Occupancy.csv   \n",
       "29                           Sonar/sonar.all-data.csv   \n",
       "30                              Spambase/Spambase.csv   \n",
       "31    Student/Students-Performance-MAT_processada.csv   \n",
       "32                     titanic/titanic_processada.csv   \n",
       "33                                    Tokyo/Tokyo.csv   \n",
       "34                                twonorm/twonorm.csv   \n",
       "35                            Vertebral/column_2C.csv   \n",
       "36                            Vertebral/column_3C.csv   \n",
       "37  Votes_Congressional/house-votes-84_processada.csv   \n",
       "38                                    wine/WineQT.csv   \n",
       "39  ['Hill/Hill_Valley_with_noise_Training.csv', '...   \n",
       "\n",
       "                        classe  \\\n",
       "0              Above/Below 50K   \n",
       "1                        Class   \n",
       "2                        class   \n",
       "3                        Class   \n",
       "4               Classification   \n",
       "5                    diagnosis   \n",
       "6                        Churn   \n",
       "7               two_year_recid   \n",
       "8   default.payment.next.month   \n",
       "9                      Outcome   \n",
       "10                    Column15   \n",
       "11                     default   \n",
       "12            SeriousDlqin2yrs   \n",
       "13                      output   \n",
       "14                         num   \n",
       "15             RiskPerformance   \n",
       "16                     surgery   \n",
       "17                      target   \n",
       "18                     defects   \n",
       "19                    selector   \n",
       "20                    Severity   \n",
       "21                       range   \n",
       "22                       Class   \n",
       "23                       Class   \n",
       "24                       Class   \n",
       "25                   Column165   \n",
       "26                       class   \n",
       "27                       Class   \n",
       "28                   Occupancy   \n",
       "29                       class   \n",
       "30                       Class   \n",
       "31                       Class   \n",
       "32                    Survived   \n",
       "33                       class   \n",
       "34                       class   \n",
       "35                       class   \n",
       "36                       class   \n",
       "37                  Class Name   \n",
       "38                     quality   \n",
       "39                       class   \n",
       "\n",
       "                                              metrica     metrica_v2  \n",
       "0                                        memoria 18gb           None  \n",
       "1   {\\n    \"Quantidade de inst\\u00e2ncias contrafa...           None  \n",
       "2   {\"Quantidade de inst\\u00e2ncias contrafactuais...           None  \n",
       "3   {\\n    \"Quantidade de instâncias contrafactuai...  zero metricas  \n",
       "4   {\"Quantidade de inst\\u00e2ncias contrafactuais...           None  \n",
       "5   {\"Quantidade de inst\\u00e2ncias contrafactuais...           None  \n",
       "6   Input contains NaN, infinity or a value too la...           None  \n",
       "7                        base de dados nao processada           None  \n",
       "8                                             memoria           None  \n",
       "9   {\"Quantidade de inst\\u00e2ncias contrafactuais...           None  \n",
       "10                                            memoria           None  \n",
       "11  {\"Quantidade de inst\\u00e2ncias contrafactuais...           None  \n",
       "12                                            memoria           None  \n",
       "13  {\"Quantidade de inst\\u00e2ncias contrafactuais...           None  \n",
       "14  {\"Quantidade de inst\\u00e2ncias contrafactuais...           None  \n",
       "15  {\"Quantidade de inst\\u00e2ncias contrafactuais...           None  \n",
       "16                array must not contain infs or NaNs           None  \n",
       "17                array must not contain infs or NaNs           None  \n",
       "18  {\"Quantidade de inst\\u00e2ncias contrafactuais...           None  \n",
       "19  {\"Quantidade de inst\\u00e2ncias contrafactuais...           None  \n",
       "20  {\"Quantidade de inst\\u00e2ncias contrafactuais...           None  \n",
       "21  {\\n    \"Quantidade de instâncias contrafactuai...           None  \n",
       "22  {\"Quantidade de inst\\u00e2ncias contrafactuais...           None  \n",
       "23  {\"Quantidade de inst\\u00e2ncias contrafactuais...           None  \n",
       "24  {\"Quantidade de inst\\u00e2ncias contrafactuais...           None  \n",
       "25                          fica dando erro de coluna           None  \n",
       "26                                            memoria           None  \n",
       "27  {\"Quantidade de inst\\u00e2ncias contrafactuais...           None  \n",
       "28  {\"Quantidade de inst\\u00e2ncias contrafactuais...           None  \n",
       "29  {\"Quantidade de inst\\u00e2ncias contrafactuais...           None  \n",
       "30  {\"Quantidade de inst\\u00e2ncias contrafactuais...           None  \n",
       "31                array must not contain infs or NaNs           None  \n",
       "32  Input contains NaN, infinity or a value too la...           None  \n",
       "33  {\"Quantidade de inst\\u00e2ncias contrafactuais...           None  \n",
       "34  {\"Quantidade de inst\\u00e2ncias contrafactuais...           None  \n",
       "35  {\"Quantidade de inst\\u00e2ncias contrafactuais...           None  \n",
       "36                                                 {}           None  \n",
       "37  {\"Quantidade de inst\\u00e2ncias contrafactuais...           None  \n",
       "38                                     demorou demais           None  \n",
       "39  {\\n    \"Quantidade de instâncias contrafactuai...  zero metricas  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups = {\n",
    "    \"Australian\": [\"Australian\"],\n",
    "}\n",
    "\n",
    "for g in groups:\n",
    "    df_map_inference_datasets = handler(str(groups[g]), 'rn')\n",
    "    \n",
    "df_map_inference_datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
